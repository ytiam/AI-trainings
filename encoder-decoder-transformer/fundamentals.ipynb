{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f091a9e",
   "metadata": {},
   "source": [
    "**How to control hallucinations in llms?**\n",
    "\n",
    "Controlling hallucinations (i.e., confident but incorrect or fabricated outputs) in Large Language Models (LLMs) is a major area of active research. Here are the most effective strategies used in practice:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Retrieval-Augmented Generation (RAG)**\n",
    "\n",
    "* Combine the LLM with a search or database retrieval system.\n",
    "* Fetch relevant documents based on the input query and pass them to the LLM as context.\n",
    "* Example: Use vector search on a knowledge base and feed top-k results with the prompt.\n",
    "\n",
    "**Tools/Libraries**: LangChain, LlamaIndex, Haystack\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Fine-Tuning / Instruction Tuning**\n",
    "\n",
    "* Train the model on task-specific or domain-specific data with a focus on factual correctness.\n",
    "* SFT or RLHF can help guide the model to align with expected, correct behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Prompt Engineering**\n",
    "\n",
    "* Use system prompts or few-shot examples that emphasize correctness.\n",
    "* Example:\n",
    "  *\"Answer only based on provided information. If unknown, say 'I don't know.'\"*\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Use of Guardrails and Constraints**\n",
    "\n",
    "* Add logic constraints post-generation to check facts, formats, or domain rules.\n",
    "* Example: Output verification against structured knowledge graphs or APIs.\n",
    "\n",
    "**Tools**: Guardrails.ai, Rebuff, Outlines\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Post-Generation Verification**\n",
    "\n",
    "* Use fact-checking models or tools to verify each statement post-generation.\n",
    "* Can also involve external API/database lookups.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Model Choice and Temperature Setting**\n",
    "\n",
    "* Use more factual LLMs like GPT-4, Claude, Gemini, or Mixtral variants.\n",
    "* Lower the temperature (e.g., 0.1‚Äì0.3) to reduce randomness and speculative output.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Chain-of-Thought or Step-by-Step Reasoning**\n",
    "\n",
    "* Encourage intermediate reasoning steps to reduce leaps of logic.\n",
    "* Example: ‚ÄúLet‚Äôs think step by step.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Hybrid Approaches**\n",
    "\n",
    "* Combine symbolic systems (like rules or logic programs) with neural generation.\n",
    "* Example: LLM generates code/query, and results are verified via execution.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know your specific use case (e.g., chat assistant, code generation, summarization), and I can suggest a targeted approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91802dec",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px; background-color: black; border: none;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0af04a",
   "metadata": {},
   "source": [
    "**By adjusting temperature, top-p or top-k, can we control hallucinations?**\n",
    "\n",
    "Yes, **adjusting `temperature`, `top-p`, and `top-k`** can influence hallucinations‚Äîbut they **don't eliminate** them. Here's how each affects generation and hallucination risk:\n",
    "\n",
    "---\n",
    "\n",
    "### üî• 1. **Temperature**\n",
    "\n",
    "* Controls randomness of token selection.\n",
    "* **Lower temperature (e.g., 0.1‚Äì0.3)**: More deterministic, safe, and factual responses.\n",
    "* **Higher temperature (e.g., 0.8‚Äì1.0+)**: More creative but prone to hallucinations.\n",
    "\n",
    "**Effect**:\n",
    "‚úÖ Lower temperature **reduces hallucination** likelihood by favoring high-probability tokens.\n",
    "‚ö†Ô∏è Too low can make outputs rigid or unhelpful.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ 2. **Top-k Sampling**\n",
    "\n",
    "* Limits the next token choices to the **top-k most probable tokens**.\n",
    "* **Low `k` (e.g., 10‚Äì50)**: Less variety, more factual.\n",
    "* **High `k` (e.g., 100‚Äì1000)**: More diverse, potentially more hallucinations.\n",
    "\n",
    "**Effect**:\n",
    "‚úÖ Small `k` restricts off-track generations.\n",
    "‚ö†Ô∏è Too small `k` can cause repetition or lack of nuance.\n",
    "\n",
    "---\n",
    "\n",
    "### üé≤ 3. **Top-p (Nucleus) Sampling**\n",
    "\n",
    "* Selects from the **smallest set of tokens whose cumulative probability ‚â• `p`**.\n",
    "* **Low `p` (e.g., 0.8)**: More focused, conservative.\n",
    "* **High `p` (e.g., 0.95+)**: Higher chance of unexpected or creative responses.\n",
    "\n",
    "**Effect**:\n",
    "‚úÖ Lower `p` improves reliability by limiting token variety.\n",
    "‚ö†Ô∏è Too low can reduce expressiveness or cut off useful completions.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary: What to Use to Reduce Hallucinations\n",
    "\n",
    "| Parameter   | Recommended Value | Effect on Hallucination |\n",
    "| ----------- | ----------------- | ----------------------- |\n",
    "| Temperature | 0.1‚Äì0.3           | Reduces                 |\n",
    "| Top-k       | 20‚Äì50             | Reduces                 |\n",
    "| Top-p       | 0.7‚Äì0.9           | Reduces                 |\n",
    "\n",
    "But **note**: These settings only **reduce randomness**, not **ensure truthfulness**. The model can still confidently generate false statements if its training data was incorrect or if it's prompted poorly.\n",
    "\n",
    "---\n",
    "\n",
    "If hallucination control is mission-critical, **combine these settings with retrieval (RAG)** or **post-verification mechanisms** for best results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba08fc",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px; background-color: black; border: none;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a93958",
   "metadata": {},
   "source": [
    "**Is there any equation involved adjusting temparature, top-p and top-k all together?**\n",
    "\n",
    "Great question. While there's **no single unified equation** that combines `temperature`, `top-p`, and `top-k` in one mathematical formula, they all operate **sequentially** in the **token sampling process**. Here‚Äôs a breakdown of how they interact:\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Token Sampling Pipeline (with `temperature`, `top-k`, `top-p`)\n",
    "\n",
    "Given a probability distribution $P(w_i)$ over tokens $w_i$, the steps are:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Apply Temperature Scaling**\n",
    "\n",
    "The logits (pre-softmax scores) $z_i$ are divided by temperature $T$:\n",
    "\n",
    "$$\n",
    "P_T(w_i) = \\frac{e^{z_i / T}}{\\sum_j e^{z_j / T}}\n",
    "$$\n",
    "\n",
    "* Lower $T$ ‚Üí sharper (peaky) distribution ‚Üí favors high-probability tokens.\n",
    "* Higher $T$ ‚Üí flatter distribution ‚Üí more randomness.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Apply Top-k Filtering**\n",
    "\n",
    "* Keep only the top **k** tokens with highest $P_T(w_i)$.\n",
    "* Renormalize the distribution over these tokens:\n",
    "\n",
    "$$\n",
    "P_{\\text{top-k}}(w_i) = \\frac{P_T(w_i)}{\\sum_{j \\in \\text{top-k}} P_T(w_j)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Apply Top-p (Nucleus) Filtering**\n",
    "\n",
    "* Sort tokens by $P_T(w_i)$ in descending order.\n",
    "* Keep the smallest set where cumulative probability ‚â• $p$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{N_p} P_T(w_i) \\geq p\n",
    "$$\n",
    "\n",
    "* Renormalize over this nucleus set (just like top-k).\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Key Points:\n",
    "\n",
    "* **Temperature** modifies the shape of the distribution **before** filtering.\n",
    "* **Top-k** limits the number of candidate tokens.\n",
    "* **Top-p** adapts the candidate pool size based on cumulative probability.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Notation Summary:\n",
    "\n",
    "```text\n",
    "Raw logits ‚Üí Temperature scaling ‚Üí Softmax\n",
    "          ‚Üì\n",
    "     Apply Top-k filter\n",
    "          ‚Üì\n",
    "     Apply Top-p filter\n",
    "          ‚Üì\n",
    "     Sample next token from this filtered distribution\n",
    "```\n",
    "\n",
    "So, while there‚Äôs no single formula like `f(temperature, top-k, top-p)`, they **combine sequentially to shape the sampling distribution**. If you want to experiment or visualize their interaction, let me know‚ÄîI can help with a simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6f7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
