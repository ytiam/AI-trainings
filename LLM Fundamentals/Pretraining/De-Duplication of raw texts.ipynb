{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d420367e",
   "metadata": {},
   "source": [
    "**Deduplication in LLM pre-training datasets** refers to the process of identifying and removing duplicate or near-duplicate text data from the training corpus before training a large language model (LLM).\n",
    "\n",
    "### Why is Deduplication Important?\n",
    "\n",
    "1. **Avoids overfitting to repeated data**: LLMs trained on duplicated text can memorize common passages, such as Wikipedia articles or popular books, leading to overfitting and reduced generalization.\n",
    "2. **Improves model quality**: Redundant data reduces the diversity of information, which can hurt performance on downstream tasks.\n",
    "3. **Reduces training cost**: Duplicate content wastes compute resources since the model sees the same content multiple times.\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Duplicates\n",
    "\n",
    "1. **Exact duplicates**: Identical documents or paragraphs repeated multiple times.\n",
    "2. **Near-duplicates**: Slight variations of the same text (e.g., with typos, formatting changes, added/removed sentences).\n",
    "3. **Template-based duplicates**: Repeated boilerplate text with minor changes (common in web data).\n",
    "\n",
    "---\n",
    "\n",
    "### Deduplication Techniques\n",
    "\n",
    "1. **Hashing-based methods**:\n",
    "\n",
    "   * Use hash functions (like SHA256) to detect exact matches.\n",
    "   * **MinHash** or **SimHash** are used for approximate deduplication.\n",
    "\n",
    "2. **Embedding similarity**:\n",
    "\n",
    "   * Generate vector embeddings for documents and compute cosine similarity.\n",
    "   * High-similarity pairs are considered near-duplicates.\n",
    "\n",
    "3. **Shingling + Jaccard similarity**:\n",
    "\n",
    "   * Break text into overlapping substrings (shingles).\n",
    "   * Compare sets using Jaccard similarity to detect overlaps.\n",
    "\n",
    "4. **Clustering**:\n",
    "\n",
    "   * Cluster similar documents and keep only representative samples.\n",
    "\n",
    "---\n",
    "\n",
    "### In Practice\n",
    "\n",
    "For example, OpenAI's LLM pre-training (like GPT-4) or Meta’s LLaMA series includes deduplication to ensure high-quality, diverse data. This is especially critical when using large web crawls (e.g., Common Crawl) where duplication is rampant.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want to see code or tools used for deduplication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f512b7e0",
   "metadata": {},
   "source": [
    "Here are **code-level techniques and tools** commonly used for deduplication in LLM pretraining pipelines, categorized by approach:\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **1. Exact Deduplication using Hashing**\n",
    "\n",
    "Fast and simple for **exact duplicates**.\n",
    "\n",
    "```python\n",
    "import hashlib\n",
    "\n",
    "def hash_document(text: str) -> str:\n",
    "    return hashlib.sha256(text.strip().encode('utf-8')).hexdigest()\n",
    "\n",
    "unique_docs = {}\n",
    "for doc in documents:\n",
    "    h = hash_document(doc)\n",
    "    if h not in unique_docs:\n",
    "        unique_docs[h] = doc\n",
    "\n",
    "deduplicated = list(unique_docs.values())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **2. MinHash (Approximate Deduplication)**\n",
    "\n",
    "Efficient for **near-duplicate detection** using **Jaccard similarity**.\n",
    "\n",
    "```python\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "def get_minhash(text, num_perm=128):\n",
    "    shingles = set(text.split())\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for shingle in shingles:\n",
    "        m.update(shingle.encode('utf8'))\n",
    "    return m\n",
    "\n",
    "# Create LSH index\n",
    "lsh = MinHashLSH(threshold=0.8, num_perm=128)\n",
    "minhashes = {}\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    m = get_minhash(doc)\n",
    "    if not lsh.query(m):\n",
    "        lsh.insert(f\"doc_{i}\", m)\n",
    "        minhashes[f\"doc_{i}\"] = doc\n",
    "\n",
    "deduplicated = list(minhashes.values())\n",
    "```\n",
    "\n",
    "Install with:\n",
    "\n",
    "```bash\n",
    "pip install datasketch\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **3. Embedding-based Deduplication**\n",
    "\n",
    "Best for **semantic similarity** (e.g., paraphrased or reworded duplicates).\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(documents, convert_to_tensor=True)\n",
    "\n",
    "# Compute pairwise cosine similarity\n",
    "cos_sim = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "# Filter out documents that are too similar (e.g., > 0.9)\n",
    "import numpy as np\n",
    "mask = np.ones(len(documents), dtype=bool)\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    if not mask[i]:\n",
    "        continue\n",
    "    for j in range(i+1, len(documents)):\n",
    "        if cos_sim[i][j] > 0.9:\n",
    "            mask[j] = False\n",
    "\n",
    "deduplicated = [doc for i, doc in enumerate(documents) if mask[i]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **4. Tools Used in Research Pipelines**\n",
    "\n",
    "| Tool / Library                                                            | Purpose                                          | Notes                                       |\n",
    "| ------------------------------------------------------------------------- | ------------------------------------------------ | ------------------------------------------- |\n",
    "| `dedup-texts` ([GitHub](https://github.com/facebookresearch/dedup-texts)) | Near-duplicate detection using SimHash           | Used by Meta in LLaMA                       |\n",
    "| `datasketch`                                                              | Locality Sensitive Hashing, MinHash              | Good for large-scale deduplication          |\n",
    "| `Cleanlab`                                                                | Dataset cleaning, includes duplication detection | Focused on data quality                     |\n",
    "| `Apache Spark` + `LSH`                                                    | Scalable deduplication on large datasets         | Used in large-scale web crawls              |\n",
    "| `FAISS`                                                                   | Vector similarity search                         | Efficient for embedding-based deduplication |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want:\n",
    "\n",
    "* A scalable pipeline example (e.g., Spark-based).\n",
    "* Deduplication tailored for JSONL datasets used in LLMs.\n",
    "* Integration into your current tokenizer or pretraining pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1687dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
