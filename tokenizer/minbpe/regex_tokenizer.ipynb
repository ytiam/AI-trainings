{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eee247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Contains the base Tokenizer class and a few common helper functions.\n",
    "The base class also contains the (common) save/load functionality.\n",
    "It would be possible to be a lot more strict about the interface and\n",
    "e.g. isolating all regex/pattern parts to the RegexTokenizer, but\n",
    "some concessions are made for simplicity.\n",
    "\"\"\"\n",
    "import unicodedata\n",
    "import regex as re\n",
    "# -----------------------------------------------------------------------------\n",
    "# a few helper functions useful for both BasicTokenizer and RegexTokenizer\n",
    "\n",
    "def get_stats(ids, counts=None):\n",
    "    \"\"\"\n",
    "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Optionally allows to update an existing dictionary of counts\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    In the list of integers (ids), replace all consecutive occurrences\n",
    "    of pair with the new integer token idx\n",
    "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# first two helper functions...\n",
    "def replace_control_characters(s: str) -> str:\n",
    "    # we don't want to print control characters\n",
    "    # which distort the output (e.g. \\n or much worse)\n",
    "    # https://stackoverflow.com/questions/4324790/removing-control-characters-from-a-string-in-python/19016117#19016117\n",
    "    # http://www.unicode.org/reports/tr44/#GC_Values_Table\n",
    "    chars = []\n",
    "    for ch in s:\n",
    "        if unicodedata.category(ch)[0] != \"C\":\n",
    "            chars.append(ch) # this character is ok\n",
    "        else:\n",
    "            chars.append(f\"\\\\u{ord(ch):04x}\") # escape\n",
    "    return \"\".join(chars)\n",
    "\n",
    "def render_token(t: bytes) -> str:\n",
    "    # pretty print a token, escaping control characters\n",
    "    s = t.decode('utf-8', errors='replace')\n",
    "    s = replace_control_characters(s)\n",
    "    return s\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# the base Tokenizer class\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Base class for Tokenizers\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # default: vocab size of 256 (all bytes), no merges, no patterns\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.pattern = \"\" # str\n",
    "        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}\n",
    "        self.vocab = self._build_vocab() # int -> bytes\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        # Tokenizer can train a vocabulary of size vocab_size from text\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Tokenizer can encode a string into a list of integers\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # Tokenizer can decode a list of integers into a string\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        # vocab is simply and deterministically derived from merges\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        for special, idx in self.special_tokens.items():\n",
    "            vocab[idx] = special.encode(\"utf-8\")\n",
    "        return vocab\n",
    "\n",
    "    def save(self, file_prefix):\n",
    "        \"\"\"\n",
    "        Saves two files: file_prefix.vocab and file_prefix.model\n",
    "        This is inspired (but not equivalent to!) sentencepiece's model saving:\n",
    "        - model file is the critical one, intended for load()\n",
    "        - vocab file is just a pretty printed version for human inspection only\n",
    "        \"\"\"\n",
    "        # write the model: to be used in load() later\n",
    "        model_file = file_prefix + \".model\"\n",
    "        with open(model_file, 'w') as f:\n",
    "            # write the version, pattern and merges, that's all that's needed\n",
    "            f.write(\"minbpe v1\\n\")\n",
    "            f.write(f\"{self.pattern}\\n\")\n",
    "            # write the special tokens, first the number of them, then each one\n",
    "            f.write(f\"{len(self.special_tokens)}\\n\")\n",
    "            for special, idx in self.special_tokens.items():\n",
    "                f.write(f\"{special} {idx}\\n\")\n",
    "            # the merges dict\n",
    "            for idx1, idx2 in self.merges:\n",
    "                f.write(f\"{idx1} {idx2}\\n\")\n",
    "        # write the vocab: for the human to look at\n",
    "        vocab_file = file_prefix + \".vocab\"\n",
    "        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for idx, token in self.vocab.items():\n",
    "                # note: many tokens may be partial utf-8 sequences\n",
    "                # and cannot be decoded into valid strings. Here we're using\n",
    "                # errors='replace' to replace them with the replacement char ï¿½.\n",
    "                # this also means that we couldn't possibly use .vocab in load()\n",
    "                # because decoding in this way is a lossy operation!\n",
    "                s = render_token(token)\n",
    "                # find the children of this token, if any\n",
    "                if idx in inverted_merges:\n",
    "                    # if this token has children, render it nicely as a merge\n",
    "                    idx0, idx1 = inverted_merges[idx]\n",
    "                    s0 = render_token(self.vocab[idx0])\n",
    "                    s1 = render_token(self.vocab[idx1])\n",
    "                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
    "                else:\n",
    "                    # otherwise this is leaf token, just print it\n",
    "                    # (this should just be the first 256 tokens, the bytes)\n",
    "                    f.write(f\"[{s}] {idx}\\n\")\n",
    "\n",
    "    def load(self, model_file):\n",
    "        \"\"\"Inverse of save() but only for the model file\"\"\"\n",
    "        assert model_file.endswith(\".model\")\n",
    "        # read the model file\n",
    "        merges = {}\n",
    "        special_tokens = {}\n",
    "        idx = 256\n",
    "        with open(model_file, 'r', encoding=\"utf-8\") as f:\n",
    "            # read the version\n",
    "            version = f.readline().strip()\n",
    "            assert version == \"minbpe v1\"\n",
    "            # read the pattern\n",
    "            self.pattern = f.readline().strip()\n",
    "            # read the special tokens\n",
    "            num_special = int(f.readline().strip())\n",
    "            for _ in range(num_special):\n",
    "                special, special_idx = f.readline().strip().split()\n",
    "                special_tokens[special] = int(special_idx)\n",
    "            # read the merges\n",
    "            for line in f:\n",
    "                idx1, idx2 = map(int, line.split())\n",
    "                merges[(idx1, idx2)] = idx\n",
    "                idx += 1\n",
    "        self.merges = merges\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab = self._build_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b65bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b457f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexTokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self, pattern=None):\n",
    "        \"\"\"\n",
    "        - pattern: optional string to override the default (GPT-4 split pattern)\n",
    "        - special_tokens: str -> int dictionary of special tokens\n",
    "          example: {'<|endoftext|>': 100257}\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
    "        self.compiled_pattern = re.compile(self.pattern)\n",
    "        self.special_tokens = {}\n",
    "        self.inverse_special_tokens = {}\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "\n",
    "        # split the text up into text chunks\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "\n",
    "        # input text preprocessing\n",
    "        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
    "\n",
    "        # iteratively merge the most common pairs to create new tokens\n",
    "        merges = {} # (int, int) -> int\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n",
    "        for i in range(num_merges):\n",
    "            # count the number of times every consecutive pair appears\n",
    "            stats = {}\n",
    "            for chunk_ids in ids:\n",
    "                # passing in stats will update it in place, adding up counts\n",
    "                get_stats(chunk_ids, stats)\n",
    "            # find the pair with the highest count\n",
    "            pair = max(stats, key=stats.get)\n",
    "            # mint a new token: assign it the next available id\n",
    "            idx = 256 + i\n",
    "            # replace all occurrences of pair in ids with idx\n",
    "            ids = [merge(chunk_ids, pair, idx) for chunk_ids in ids]\n",
    "            # save the merge\n",
    "            merges[pair] = idx\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            # prints\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
    "\n",
    "        # save class variables\n",
    "        self.merges = merges # used in encode()\n",
    "        self.vocab = vocab   # used in decode()\n",
    "\n",
    "    def register_special_tokens(self, special_tokens):\n",
    "        # special_tokens is a dictionary of str -> int\n",
    "        # example: {\"<|endoftext|>\": 100257}\n",
    "        self.special_tokens = special_tokens\n",
    "        self.inverse_special_tokens = {v: k for k, v in special_tokens.items()}\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # given ids (list of integers), return Python string\n",
    "        part_bytes = []\n",
    "        for idx in ids:\n",
    "            if idx in self.vocab:\n",
    "                part_bytes.append(self.vocab[idx])\n",
    "            elif idx in self.inverse_special_tokens:\n",
    "                part_bytes.append(self.inverse_special_tokens[idx].encode(\"utf-8\"))\n",
    "            else:\n",
    "                raise ValueError(f\"invalid token id: {idx}\")\n",
    "        text_bytes = b\"\".join(part_bytes)\n",
    "        text = text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "\n",
    "    def _encode_chunk(self, text_bytes):\n",
    "        # return the token ids\n",
    "        # let's begin. first, convert all bytes to integers in range 0..255\n",
    "        ids = list(text_bytes)\n",
    "        while len(ids) >= 2:\n",
    "            # find the pair with the lowest merge index\n",
    "            stats = get_stats(ids)\n",
    "            pair = min(stats, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            # subtle: if there are no more merges available, the key will\n",
    "            # result in an inf for every single pair, and the min will be\n",
    "            # just the first pair in the list, arbitrarily\n",
    "            # we can detect this terminating case by a membership check\n",
    "            if pair not in self.merges:\n",
    "                break # nothing else can be merged anymore\n",
    "            # otherwise let's merge the best pair (lowest merge index)\n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(ids, pair, idx)\n",
    "        return ids\n",
    "\n",
    "    def encode_ordinary(self, text):\n",
    "        \"\"\"Encoding that ignores any special tokens.\"\"\"\n",
    "        # split text into chunks of text by categories defined in regex pattern\n",
    "        text_chunks = re.findall(self.compiled_pattern, text)\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "        for chunk in text_chunks:\n",
    "            chunk_bytes = chunk.encode(\"utf-8\") # raw bytes\n",
    "            chunk_ids = self._encode_chunk(chunk_bytes)\n",
    "            ids.extend(chunk_ids)\n",
    "        return ids\n",
    "\n",
    "    def encode(self, text, allowed_special=\"none_raise\"):\n",
    "        \"\"\"\n",
    "        Unlike encode_ordinary, this function handles special tokens.\n",
    "        allowed_special: can be \"all\"|\"none\"|\"none_raise\" or a custom set of special tokens\n",
    "        if none_raise, then an error is raised if any special token is encountered in text\n",
    "        this is the default tiktoken behavior right now as well\n",
    "        any other behavior is either annoying, or a major footgun\n",
    "        \"\"\"\n",
    "        # decode the user desire w.r.t. handling of special tokens\n",
    "        special = None\n",
    "        if allowed_special == \"all\":\n",
    "            special = self.special_tokens\n",
    "        elif allowed_special == \"none\":\n",
    "            special = {}\n",
    "        elif allowed_special == \"none_raise\":\n",
    "            special = {}\n",
    "            assert all(token not in text for token in self.special_tokens)\n",
    "        elif isinstance(allowed_special, set):\n",
    "            special = {k: v for k, v in self.special_tokens.items() if k in allowed_special}\n",
    "        else:\n",
    "            raise ValueError(f\"allowed_special={allowed_special} not understood\")\n",
    "        if not special:\n",
    "            # shortcut: if no special tokens, just use the ordinary encoding\n",
    "            return self.encode_ordinary(text)\n",
    "        # otherwise, we have to be careful with potential special tokens in text\n",
    "        # we handle special tokens by splitting the text\n",
    "        # based on the occurrence of any exact match with any of the special tokens\n",
    "        # we can use re.split for this. note that surrounding the pattern with ()\n",
    "        # makes it into a capturing group, so the special tokens will be included\n",
    "        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
    "        special_chunks = re.split(special_pattern, text)\n",
    "        # now all the special characters are separated from the rest of the text\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "        for part in special_chunks:\n",
    "            if part in special:\n",
    "                # this is a special token, encode it separately as a special case\n",
    "                ids.append(special[part])\n",
    "            else:\n",
    "                # this is an ordinary sequence, encode it normally\n",
    "                ids.extend(self.encode_ordinary(part))\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a570627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_tok = RegexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81238bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open some text and train a vocab of 512 tokens\n",
    "text = open(\"tests/taylorswift.txt\", \"r\", encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "372c5d9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 1/256: (101, 114) -> 256 (b'er') had 2359 occurrences\n",
      "merge 2/256: (50, 48) -> 257 (b'20') had 2187 occurrences\n",
      "merge 3/256: (111, 114) -> 258 (b'or') had 2076 occurrences\n",
      "merge 4/256: (105, 110) -> 259 (b'in') had 2006 occurrences\n",
      "merge 5/256: (101, 100) -> 260 (b'ed') had 1876 occurrences\n",
      "merge 6/256: (32, 116) -> 261 (b' t') had 1824 occurrences\n",
      "merge 7/256: (111, 110) -> 262 (b'on') had 1815 occurrences\n",
      "merge 8/256: (104, 101) -> 263 (b'he') had 1772 occurrences\n",
      "merge 9/256: (32, 83) -> 264 (b' S') had 1633 occurrences\n",
      "merge 10/256: (97, 114) -> 265 (b'ar') had 1519 occurrences\n",
      "merge 11/256: (97, 110) -> 266 (b'an') had 1487 occurrences\n",
      "merge 12/256: (32, 65) -> 267 (b' A') had 1335 occurrences\n",
      "merge 13/256: (261, 263) -> 268 (b' the') had 1169 occurrences\n",
      "merge 14/256: (97, 108) -> 269 (b'al') had 1164 occurrences\n",
      "merge 15/256: (114, 105) -> 270 (b'ri') had 1156 occurrences\n",
      "merge 16/256: (118, 260) -> 271 (b'ved') had 1104 occurrences\n",
      "merge 17/256: (115, 116) -> 272 (b'st') had 1089 occurrences\n",
      "merge 18/256: (119, 105) -> 273 (b'wi') had 1049 occurrences\n",
      "merge 19/256: (32, 82) -> 274 (b' R') had 1045 occurrences\n",
      "merge 20/256: (257, 49) -> 275 (b'201') had 981 occurrences\n",
      "merge 21/256: (32, 102) -> 276 (b' f') had 967 occurrences\n",
      "merge 22/256: (257, 50) -> 277 (b'202') had 952 occurrences\n",
      "merge 23/256: (32, 84) -> 278 (b' T') had 934 occurrences\n",
      "merge 24/256: (102, 116) -> 279 (b'ft') had 934 occurrences\n",
      "merge 25/256: (97, 121) -> 280 (b'ay') had 900 occurrences\n",
      "merge 26/256: (32, 34) -> 281 (b' \"') had 882 occurrences\n",
      "merge 27/256: (273, 279) -> 282 (b'wift') had 870 occurrences\n",
      "merge 28/256: (101, 116) -> 283 (b'et') had 852 occurrences\n",
      "merge 29/256: (264, 282) -> 284 (b' Swift') had 817 occurrences\n",
      "merge 30/256: (99, 104) -> 285 (b'ch') had 797 occurrences\n",
      "merge 31/256: (98, 256) -> 286 (b'ber') had 797 occurrences\n",
      "merge 32/256: (97, 116) -> 287 (b'at') had 790 occurrences\n",
      "merge 33/256: (111, 109) -> 288 (b'om') had 789 occurrences\n",
      "merge 34/256: (101, 115) -> 289 (b'es') had 743 occurrences\n",
      "merge 35/256: (101, 110) -> 290 (b'en') had 724 occurrences\n",
      "merge 36/256: (101, 109) -> 291 (b'em') had 699 occurrences\n",
      "merge 37/256: (34, 46) -> 292 (b'\".') had 693 occurrences\n",
      "merge 38/256: (32, 40) -> 293 (b' (') had 685 occurrences\n",
      "merge 39/256: (46, 10) -> 294 (b'.\\n') had 684 occurrences\n",
      "merge 40/256: (259, 103) -> 295 (b'ing') had 684 occurrences\n",
      "merge 41/256: (108, 258) -> 296 (b'lor') had 680 occurrences\n",
      "merge 42/256: (32, 77) -> 297 (b' M') had 662 occurrences\n",
      "merge 43/256: (105, 103) -> 298 (b'ig') had 655 occurrences\n",
      "merge 44/256: (32, 262) -> 299 (b' on') had 654 occurrences\n",
      "merge 45/256: (280, 296) -> 300 (b'aylor') had 650 occurrences\n",
      "merge 46/256: (108, 108) -> 301 (b'll') had 649 occurrences\n",
      "merge 47/256: (270, 101) -> 302 (b'rie') had 646 occurrences\n",
      "merge 48/256: (274, 283) -> 303 (b' Ret') had 643 occurrences\n",
      "merge 49/256: (303, 302) -> 304 (b' Retrie') had 639 occurrences\n",
      "merge 50/256: (304, 271) -> 305 (b' Retrieved') had 639 occurrences\n",
      "merge 51/256: (32, 115) -> 306 (b' s') had 609 occurrences\n",
      "merge 52/256: (105, 99) -> 307 (b'ic') had 593 occurrences\n",
      "merge 53/256: (266, 100) -> 308 (b'and') had 573 occurrences\n",
      "merge 54/256: (111, 117) -> 309 (b'ou') had 555 occurrences\n",
      "merge 55/256: (101, 99) -> 310 (b'ec') had 554 occurrences\n",
      "merge 56/256: (32, 97) -> 311 (b' a') had 553 occurrences\n",
      "merge 57/256: (41, 46) -> 312 (b').') had 533 occurrences\n",
      "merge 58/256: (114, 288) -> 313 (b'rom') had 532 occurrences\n",
      "merge 59/256: (32, 66) -> 314 (b' B') had 530 occurrences\n",
      "merge 60/256: (291, 286) -> 315 (b'ember') had 529 occurrences\n",
      "merge 61/256: (32, 111) -> 316 (b' o') had 527 occurrences\n",
      "merge 62/256: (276, 313) -> 317 (b' from') had 503 occurrences\n",
      "merge 63/256: (267, 114) -> 318 (b' Ar') had 495 occurrences\n",
      "merge 64/256: (32, 308) -> 319 (b' and') had 480 occurrences\n",
      "merge 65/256: (32, 67) -> 320 (b' C') had 472 occurrences\n",
      "merge 66/256: (32, 78) -> 321 (b' N') had 472 occurrences\n",
      "merge 67/256: (32, 258) -> 322 (b' or') had 465 occurrences\n",
      "merge 68/256: (285, 105) -> 323 (b'chi') had 458 occurrences\n",
      "merge 69/256: (32, 74) -> 324 (b' J') had 457 occurrences\n",
      "merge 70/256: (259, 269) -> 325 (b'inal') had 456 occurrences\n",
      "merge 71/256: (322, 298) -> 326 (b' orig') had 448 occurrences\n",
      "merge 72/256: (326, 325) -> 327 (b' original') had 446 occurrences\n",
      "merge 73/256: (318, 323) -> 328 (b' Archi') had 440 occurrences\n",
      "merge 74/256: (328, 271) -> 329 (b' Archived') had 440 occurrences\n",
      "merge 75/256: (316, 102) -> 330 (b' of') had 439 occurrences\n",
      "merge 76/256: (32, 104) -> 331 (b' h') had 432 occurrences\n",
      "merge 77/256: (32, 259) -> 332 (b' in') had 404 occurrences\n",
      "merge 78/256: (114, 101) -> 333 (b're') had 403 occurrences\n",
      "merge 79/256: (84, 300) -> 334 (b'Taylor') had 392 occurrences\n",
      "merge 80/256: (105, 116) -> 335 (b'it') had 386 occurrences\n",
      "merge 81/256: (97, 115) -> 336 (b'as') had 384 occurrences\n",
      "merge 82/256: (32, 112) -> 337 (b' p') had 381 occurrences\n",
      "merge 83/256: (105, 262) -> 338 (b'ion') had 381 occurrences\n",
      "merge 84/256: (32, 68) -> 339 (b' D') had 380 occurrences\n",
      "merge 85/256: (32, 119) -> 340 (b' w') had 380 occurrences\n",
      "merge 86/256: (265, 100) -> 341 (b'ard') had 374 occurrences\n",
      "merge 87/256: (105, 301) -> 342 (b'ill') had 371 occurrences\n",
      "merge 88/256: (39, 115) -> 343 (b\"'s\") had 368 occurrences\n",
      "merge 89/256: (32, 109) -> 344 (b' m') had 359 occurrences\n",
      "merge 90/256: (32, 70) -> 345 (b' F') had 356 occurrences\n",
      "merge 91/256: (32, 87) -> 346 (b' W') had 353 occurrences\n",
      "merge 92/256: (108, 101) -> 347 (b'le') had 345 occurrences\n",
      "merge 93/256: (261, 111) -> 348 (b' to') had 340 occurrences\n",
      "merge 94/256: (32, 99) -> 349 (b' c') had 340 occurrences\n",
      "merge 95/256: (46, 91) -> 350 (b'.[') had 332 occurrences\n",
      "merge 96/256: (111, 118) -> 351 (b'ov') had 317 occurrences\n",
      "merge 97/256: (108, 121) -> 352 (b'ly') had 314 occurrences\n",
      "merge 98/256: (117, 115) -> 353 (b'us') had 312 occurrences\n",
      "merge 99/256: (32, 72) -> 354 (b' H') had 310 occurrences\n",
      "merge 100/256: (105, 115) -> 355 (b'is') had 307 occurrences\n",
      "merge 101/256: (32, 80) -> 356 (b' P') had 302 occurrences\n",
      "merge 102/256: (116, 104) -> 357 (b'th') had 290 occurrences\n",
      "merge 103/256: (99, 116) -> 358 (b'ct') had 282 occurrences\n",
      "merge 104/256: (117, 109) -> 359 (b'um') had 281 occurrences\n",
      "merge 105/256: (32, 98) -> 360 (b' b') had 280 occurrences\n",
      "merge 106/256: (32, 71) -> 361 (b' G') had 277 occurrences\n",
      "merge 107/256: (265, 121) -> 362 (b'ary') had 267 occurrences\n",
      "merge 108/256: (32, 73) -> 363 (b' I') had 264 occurrences\n",
      "merge 109/256: (278, 300) -> 364 (b' Taylor') had 255 occurrences\n",
      "merge 110/256: (105, 272) -> 365 (b'ist') had 248 occurrences\n",
      "merge 111/256: (32, 100) -> 366 (b' d') had 247 occurrences\n",
      "merge 112/256: (97, 109) -> 367 (b'am') had 247 occurrences\n",
      "merge 113/256: (32, 79) -> 368 (b' O') had 247 occurrences\n",
      "merge 114/256: (111, 112) -> 369 (b'op') had 243 occurrences\n",
      "merge 115/256: (324, 117) -> 370 (b' Ju') had 239 occurrences\n",
      "merge 116/256: (331, 256) -> 371 (b' her') had 235 occurrences\n",
      "merge 117/256: (32, 76) -> 372 (b' L') had 228 occurrences\n",
      "merge 118/256: (117, 272) -> 373 (b'ust') had 228 occurrences\n",
      "merge 119/256: (97, 100) -> 374 (b'ad') had 225 occurrences\n",
      "merge 120/256: (278, 263) -> 375 (b' The') had 225 occurrences\n",
      "merge 121/256: (117, 362) -> 376 (b'uary') had 225 occurrences\n",
      "merge 122/256: (290, 116) -> 377 (b'ent') had 223 occurrences\n",
      "merge 123/256: (353, 307) -> 378 (b'usic') had 221 occurrences\n",
      "merge 124/256: (351, 315) -> 379 (b'ovember') had 221 occurrences\n",
      "merge 125/256: (101, 119) -> 380 (b'ew') had 216 occurrences\n",
      "merge 126/256: (256, 115) -> 381 (b'ers') had 215 occurrences\n",
      "merge 127/256: (265, 116) -> 382 (b'art') had 204 occurrences\n",
      "merge 128/256: (105, 100) -> 383 (b'id') had 204 occurrences\n",
      "merge 129/256: (32, 69) -> 384 (b' E') had 203 occurrences\n",
      "merge 130/256: (101, 108) -> 385 (b'el') had 203 occurrences\n",
      "merge 131/256: (262, 103) -> 386 (b'ong') had 202 occurrences\n",
      "merge 132/256: (105, 109) -> 387 (b'im') had 202 occurrences\n",
      "merge 133/256: (111, 286) -> 388 (b'ober') had 202 occurrences\n",
      "merge 134/256: (101, 112) -> 389 (b'ep') had 201 occurrences\n",
      "merge 135/256: (276, 258) -> 390 (b' for') had 199 occurrences\n",
      "merge 136/256: (358, 388) -> 391 (b'ctober') had 198 occurrences\n",
      "merge 137/256: (98, 111) -> 392 (b'bo') had 197 occurrences\n",
      "merge 138/256: (314, 342) -> 393 (b' Bill') had 196 occurrences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 139/256: (310, 315) -> 394 (b'ecember') had 195 occurrences\n",
      "merge 140/256: (264, 116) -> 395 (b' St') had 195 occurrences\n",
      "merge 141/256: (392, 341) -> 396 (b'board') had 192 occurrences\n",
      "merge 142/256: (111, 119) -> 397 (b'ow') had 186 occurrences\n",
      "merge 143/256: (117, 103) -> 398 (b'ug') had 185 occurrences\n",
      "merge 144/256: (111, 116) -> 399 (b'ot') had 184 occurrences\n",
      "merge 145/256: (393, 396) -> 400 (b' Billboard') had 184 occurrences\n",
      "merge 146/256: (49, 48) -> 401 (b'10') had 183 occurrences\n",
      "merge 147/256: (110, 116) -> 402 (b'nt') had 182 occurrences\n",
      "merge 148/256: (110, 101) -> 403 (b'ne') had 182 occurrences\n",
      "merge 149/256: (101, 265) -> 404 (b'ear') had 179 occurrences\n",
      "merge 150/256: (309, 114) -> 405 (b'our') had 179 occurrences\n",
      "merge 151/256: (270, 116) -> 406 (b'rit') had 177 occurrences\n",
      "merge 152/256: (105, 114) -> 407 (b'ir') had 176 occurrences\n",
      "merge 153/256: (98, 359) -> 408 (b'bum') had 172 occurrences\n",
      "merge 154/256: (117, 114) -> 409 (b'ur') had 171 occurrences\n",
      "merge 155/256: (287, 338) -> 410 (b'ation') had 171 occurrences\n",
      "merge 156/256: (257, 48) -> 411 (b'200') had 171 occurrences\n",
      "merge 157/256: (111, 108) -> 412 (b'ol') had 168 occurrences\n",
      "merge 158/256: (289, 115) -> 413 (b'ess') had 166 occurrences\n",
      "merge 159/256: (32, 114) -> 414 (b' r') had 164 occurrences\n",
      "merge 160/256: (93, 91) -> 415 (b'][') had 164 occurrences\n",
      "merge 161/256: (398, 373) -> 416 (b'ugust') had 164 occurrences\n",
      "merge 162/256: (32, 86) -> 417 (b' V') had 161 occurrences\n",
      "merge 163/256: (32, 39) -> 418 (b\" '\") had 160 occurrences\n",
      "merge 164/256: (101, 98) -> 419 (b'eb') had 157 occurrences\n",
      "merge 165/256: (32, 269) -> 420 (b' al') had 153 occurrences\n",
      "merge 166/256: (105, 118) -> 421 (b'iv') had 152 occurrences\n",
      "merge 167/256: (32, 89) -> 422 (b' Y') had 152 occurrences\n",
      "merge 168/256: (117, 116) -> 423 (b'ut') had 151 occurrences\n",
      "merge 169/256: (32, 273) -> 424 (b' wi') had 150 occurrences\n",
      "merge 170/256: (114, 121) -> 425 (b'ry') had 149 occurrences\n",
      "merge 171/256: (101, 272) -> 426 (b'est') had 144 occurrences\n",
      "merge 172/256: (424, 357) -> 427 (b' with') had 143 occurrences\n",
      "merge 173/256: (114, 97) -> 428 (b'ra') had 141 occurrences\n",
      "merge 174/256: (339, 394) -> 429 (b' December') had 140 occurrences\n",
      "merge 175/256: (321, 379) -> 430 (b' November') had 138 occurrences\n",
      "merge 176/256: (32, 287) -> 431 (b' at') had 133 occurrences\n",
      "merge 177/256: (32, 333) -> 432 (b' re') had 130 occurrences\n",
      "merge 178/256: (370, 352) -> 433 (b' July') had 129 occurrences\n",
      "merge 179/256: (261, 104) -> 434 (b' th') had 127 occurrences\n",
      "merge 180/256: (32, 110) -> 435 (b' n') had 127 occurrences\n",
      "merge 181/256: (267, 416) -> 436 (b' August') had 127 occurrences\n",
      "merge 182/256: (258, 100) -> 437 (b'ord') had 126 occurrences\n",
      "merge 183/256: (119, 341) -> 438 (b'ward') had 125 occurrences\n",
      "merge 184/256: (49, 57) -> 439 (b'19') had 125 occurrences\n",
      "merge 185/256: (32, 75) -> 440 (b' K') had 125 occurrences\n",
      "merge 186/256: (368, 391) -> 441 (b' October') had 125 occurrences\n",
      "merge 187/256: (32, 108) -> 442 (b' l') had 123 occurrences\n",
      "merge 188/256: (111, 301) -> 443 (b'oll') had 122 occurrences\n",
      "merge 189/256: (112, 270) -> 444 (b'pri') had 122 occurrences\n",
      "merge 190/256: (117, 108) -> 445 (b'ul') had 121 occurrences\n",
      "merge 191/256: (389, 116) -> 446 (b'ept') had 121 occurrences\n",
      "merge 192/256: (297, 378) -> 447 (b' Music') had 119 occurrences\n",
      "merge 193/256: (32, 85) -> 448 (b' U') had 119 occurrences\n",
      "merge 194/256: (111, 115) -> 449 (b'os') had 119 occurrences\n",
      "merge 195/256: (419, 114) -> 450 (b'ebr') had 119 occurrences\n",
      "merge 196/256: (311, 115) -> 451 (b' as') had 118 occurrences\n",
      "merge 197/256: (110, 100) -> 452 (b'nd') had 118 occurrences\n",
      "merge 198/256: (44, 91) -> 453 (b',[') had 118 occurrences\n",
      "merge 199/256: (450, 376) -> 454 (b'ebruary') had 118 occurrences\n",
      "merge 200/256: (321, 380) -> 455 (b' New') had 117 occurrences\n",
      "merge 201/256: (32, 272) -> 456 (b' st') had 116 occurrences\n",
      "merge 202/256: (111, 100) -> 457 (b'od') had 115 occurrences\n",
      "merge 203/256: (97, 107) -> 458 (b'ak') had 115 occurrences\n",
      "merge 204/256: (444, 108) -> 459 (b'pril') had 115 occurrences\n",
      "merge 205/256: (309, 402) -> 460 (b'ount') had 114 occurrences\n",
      "merge 206/256: (320, 104) -> 461 (b' Ch') had 114 occurrences\n",
      "merge 207/256: (99, 101) -> 462 (b'ce') had 113 occurrences\n",
      "merge 208/256: (446, 315) -> 463 (b'eptember') had 112 occurrences\n",
      "merge 209/256: (420, 408) -> 464 (b' album') had 110 occurrences\n",
      "merge 210/256: (105, 108) -> 465 (b'il') had 109 occurrences\n",
      "merge 211/256: (267, 438) -> 466 (b' Award') had 108 occurrences\n",
      "merge 212/256: (310, 437) -> 467 (b'ecord') had 108 occurrences\n",
      "merge 213/256: (266, 376) -> 468 (b'anuary') had 107 occurrences\n",
      "merge 214/256: (49, 51) -> 469 (b'13') had 106 occurrences\n",
      "merge 215/256: (460, 425) -> 470 (b'ountry') had 106 occurrences\n",
      "merge 216/256: (109, 256) -> 471 (b'mer') had 105 occurrences\n",
      "merge 217/256: (262, 101) -> 472 (b'one') had 105 occurrences\n",
      "merge 218/256: (32, 103) -> 473 (b' g') had 104 occurrences\n",
      "merge 219/256: (50, 49) -> 474 (b'21') had 103 occurrences\n",
      "merge 220/256: (265, 285) -> 475 (b'arch') had 103 occurrences\n",
      "merge 221/256: (111, 99) -> 476 (b'oc') had 101 occurrences\n",
      "merge 222/256: (310, 116) -> 477 (b'ect') had 100 occurrences\n",
      "merge 223/256: (105, 97) -> 478 (b'ia') had 98 occurrences\n",
      "merge 224/256: (118, 256) -> 479 (b'ver') had 98 occurrences\n",
      "merge 225/256: (226, 128) -> 480 (b'\\xe2\\x80') had 97 occurrences\n",
      "merge 226/256: (264, 263) -> 481 (b' She') had 96 occurrences\n",
      "merge 227/256: (297, 280) -> 482 (b' May') had 96 occurrences\n",
      "merge 228/256: (267, 108) -> 483 (b' Al') had 95 occurrences\n",
      "merge 229/256: (50, 51) -> 484 (b'23') had 95 occurrences\n",
      "merge 230/256: (370, 403) -> 485 (b' June') had 95 occurrences\n",
      "merge 231/256: (344, 378) -> 486 (b' music') had 94 occurrences\n",
      "merge 232/256: (49, 56) -> 487 (b'18') had 93 occurrences\n",
      "merge 233/256: (278, 387) -> 488 (b' Tim') had 91 occurrences\n",
      "merge 234/256: (421, 101) -> 489 (b'ive') had 90 occurrences\n",
      "merge 235/256: (345, 454) -> 490 (b' February') had 90 occurrences\n",
      "merge 236/256: (101, 101) -> 491 (b'ee') had 89 occurrences\n",
      "merge 237/256: (105, 266) -> 492 (b'ian') had 89 occurrences\n",
      "merge 238/256: (50, 52) -> 493 (b'24') had 89 occurrences\n",
      "merge 239/256: (471, 307) -> 494 (b'meric') had 88 occurrences\n",
      "merge 240/256: (49, 50) -> 495 (b'12') had 87 occurrences\n",
      "merge 241/256: (264, 463) -> 496 (b' September') had 87 occurrences\n",
      "merge 242/256: (306, 295) -> 497 (b' sing') had 86 occurrences\n",
      "merge 243/256: (32, 118) -> 498 (b' v') had 86 occurrences\n",
      "merge 244/256: (340, 336) -> 499 (b' was') had 86 occurrences\n",
      "merge 245/256: (267, 459) -> 500 (b' April') had 86 occurrences\n",
      "merge 246/256: (49, 55) -> 501 (b'17') had 84 occurrences\n",
      "merge 247/256: (50, 50) -> 502 (b'22') had 84 occurrences\n",
      "merge 248/256: (111, 103) -> 503 (b'og') had 83 occurrences\n",
      "merge 249/256: (119, 406) -> 504 (b'writ') had 83 occurrences\n",
      "merge 250/256: (78, 379) -> 505 (b'November') had 83 occurrences\n",
      "merge 251/256: (306, 386) -> 506 (b' song') had 82 occurrences\n",
      "merge 252/256: (49, 52) -> 507 (b'14') had 82 occurrences\n",
      "merge 253/256: (298, 104) -> 508 (b'igh') had 82 occurrences\n",
      "merge 254/256: (108, 100) -> 509 (b'ld') had 82 occurrences\n",
      "merge 255/256: (93, 10) -> 510 (b']\\n') had 82 occurrences\n",
      "merge 256/256: (306, 263) -> 511 (b' she') had 82 occurrences\n"
     ]
    }
   ],
   "source": [
    "reg_tok.train(text, vocab_size=512, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6fa7734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am Atanu'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_tok.decode(reg_tok.encode(\"I am Atanu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293cb2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
