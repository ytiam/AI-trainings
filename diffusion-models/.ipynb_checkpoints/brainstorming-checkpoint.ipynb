{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b4106ed",
   "metadata": {},
   "source": [
    "### In denoising diffusion model, why the concept of time is added? I mean if I simply train a model to predict the noise in the image, what can't be achieved \n",
    "\n",
    "Great question ‚Äî this gets at the **core idea behind diffusion models**.\n",
    "\n",
    "You're asking:\n",
    "\n",
    "> *Why do we need the concept of time steps? Why not just train a model to remove noise from images directly in one go?*\n",
    "\n",
    "Let‚Äôs break it down:\n",
    "\n",
    "---\n",
    "\n",
    "## üß† What ‚Äútime‚Äù means in a Denoising Diffusion Model\n",
    "\n",
    "* Time in this context is **not real clock time**, but a **discrete index** `t = 0 to T`.\n",
    "* It refers to how **much noise** has been added to an image.\n",
    "* At **t=0**, you have a clean image.\n",
    "  At **t=T**, it‚Äôs pure noise.\n",
    "  Intermediate timesteps have partially noised images.\n",
    "\n",
    "So, the ‚Äútime‚Äù encodes **how corrupted the image is**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why we need the time conditioning\n",
    "\n",
    "Imagine trying to train a model `f(x_noisy)` to remove noise, without telling it **how much noise** is in the image.\n",
    "\n",
    "> That‚Äôs like asking someone to fix a photo but not telling them if it‚Äôs slightly blurry or totally distorted.\n",
    "> They don‚Äôt know **how aggressive** the fix should be.\n",
    "\n",
    "So we instead train the model to predict noise **conditioned on time `t`**:\n",
    "\n",
    "```python\n",
    "Œµ_theta(x_t, t) ‚âà Œµ  # Predict noise added at timestep t\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "* `x_t` is the noisy image at time `t`\n",
    "* `t` tells the model how corrupted the input is\n",
    "* `Œµ` is the known noise added (during training)\n",
    "\n",
    "This way, the model **learns different noise patterns for different noise levels**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß® What goes wrong if you remove time?\n",
    "\n",
    "If you remove time conditioning and just try to denoise directly:\n",
    "\n",
    "1. The model gets inputs at different noise levels ‚Äî but doesn't know **which level**.\n",
    "2. It might over-denoise or under-denoise.\n",
    "3. This leads to **unstable training** and **poor sample quality**.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Think of it like curriculum learning\n",
    "\n",
    "* Training with timestep `t` is like saying:\n",
    "\n",
    "  * At `t=900`, just barely reduce heavy noise\n",
    "  * At `t=100`, apply fine-grained denoising\n",
    "* Without `t`, the model has to do **all denoising at once**, blindly.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå So to summarize:\n",
    "\n",
    "| Question                | Answer                                                                                                                                |\n",
    "| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| Why is \"time\" added?    | To let the model know **how much noise** was added, so it can denoise accordingly.                                                    |\n",
    "| What if we remove time? | The model can't distinguish between light and heavy noise ‚Äî results get unstable and poor.                                            |\n",
    "| Is time continuous?     | Often modeled as discrete steps (e.g., 0 to 1000), but continuous-time variants exist too (e.g., DDIM, continuous-time score models). |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like an **intuition with code/visuals** or want to explore **score-based diffusion** which builds on this further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c74d16",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c462e",
   "metadata": {},
   "source": [
    "### Understood, but during model serving or prediction time, how do we pass image with time then? I mean how it make sense?\n",
    "\n",
    "Fantastic follow-up ‚Äî you're thinking in the right direction.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Key clarification:\n",
    "\n",
    "At **inference/serving time**, we **don't pass any real image** into the model.\n",
    "We start from **pure noise** and **generate** an image **from scratch** using the model.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ The full prediction process:\n",
    "\n",
    "### You‚Äôre not denoising a noisy real image.\n",
    "\n",
    "You're **sampling** from the model using this loop:\n",
    "\n",
    "```python\n",
    "img = torch.randn((1, 3, H, W))  # Start from pure noise\n",
    "for t in reversed(range(T)):    # T ‚Üí 0\n",
    "    pred_noise = model(img, t)  # Pass image + timestep\n",
    "    img = denoise_step(img, pred_noise, t)  # Estimate x_{t-1}\n",
    "```\n",
    "\n",
    "### So at **each step**, we explicitly pass `t` to the model.\n",
    "\n",
    "Why?\n",
    "\n",
    "* Because the model is trained to predict the noise **conditioned on the current time step**.\n",
    "* It knows how to denoise an image at `t=999`, `t=500`, `t=0` ‚Äî **each one is a different job**.\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Model Input During Serving:\n",
    "\n",
    "For each step in reverse sampling, the model is given:\n",
    "\n",
    "* `x_t`: current noisy image\n",
    "* `t`: current timestep (scalar or embedding)\n",
    "\n",
    "So model call looks like:\n",
    "\n",
    "```python\n",
    "pred_noise = model(x_t, t)  # \"How much noise should be removed at this time step?\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Analogy:\n",
    "\n",
    "Think of the model as a \"photo restorer\" who's trained to handle:\n",
    "\n",
    "* Very damaged images (at t=1000)\n",
    "* Slightly damaged ones (at t=10)\n",
    "* Near-perfect touch-ups (at t=1)\n",
    "\n",
    "You need to **tell them how bad the image currently is** (via `t`) so they apply the right level of restoration.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î What if I want to denoise a custom noisy image (e.g., from the real world)?\n",
    "\n",
    "That‚Äôs a different use case:\n",
    "\n",
    "* Diffusion models aren‚Äôt meant for arbitrary-noise removal.\n",
    "* They‚Äôre trained with **very specific Gaussian noise schedules**.\n",
    "* You‚Äôd need to **estimate which timestep t your noisy image matches** ‚Äî which is a hard problem.\n",
    "\n",
    "For tasks like that, **denoising autoencoders** or other image restoration networks are more appropriate.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Final Summary:\n",
    "\n",
    "| During Training                    | During Inference                     |\n",
    "| ---------------------------------- | ------------------------------------ |\n",
    "| Start from real image              | Start from noise                     |\n",
    "| Add noise at time `t`              | Remove noise step by step            |\n",
    "| Train to predict noise at each `t` | Predict and remove noise at each `t` |\n",
    "| Input: `(x_t, t)`                  | Input: `(x_t, t)`                    |\n",
    "\n",
    "Let me know if you want to see the code of the reverse sampling loop with all pieces explained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47980f93",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec20cb7b",
   "metadata": {},
   "source": [
    "### What are the theoretical and real life applications of a de-noising diffusion model?\n",
    "\n",
    "Excellent question ‚Äî and you're right to ask this, because **DDPMs (Denoising Diffusion Probabilistic Models)** were originally **unconditional image generators**. So how do they power **Text-to-Image generation**, like DALL¬∑E 2, Imagen, or Stable Diffusion?\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Core Idea: *Conditional Diffusion*\n",
    "\n",
    "> Text-to-Image = **Diffusion Model** + **Conditioning on text**\n",
    "\n",
    "DDPMs are **flexible**: we can condition the generation process on **any auxiliary signal**, like a text embedding.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ How it's done:\n",
    "\n",
    "Let‚Äôs break it down step-by-step.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Train a Conditional Diffusion Model**\n",
    "\n",
    "The standard DDPM loss is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{simple}} = \\mathbb{E}_{x_0, \\epsilon, t} \\left[ \\| \\epsilon - \\epsilon_\\theta(x_t, t) \\|^2 \\right]\n",
    "$$\n",
    "\n",
    "In **Text-to-Image**, we **condition** the noise prediction model on a text embedding `c`:\n",
    "\n",
    "$$\n",
    "\\epsilon_\\theta(x_t, t, c)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* `x_t`: noisy image at timestep `t`\n",
    "* `c`: conditioning vector (text embedding from CLIP or T5)\n",
    "* `t`: timestep\n",
    "* `Œµ`: true noise\n",
    "* `Œµ_Œ∏`: predicted noise\n",
    "\n",
    "So now the model learns:\n",
    "\n",
    "* **\"At noise level `t`, and given text description `c`, what should the noise look like?\"**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Text Embedding via Pretrained Language/Image Model**\n",
    "\n",
    "* The text prompt (e.g., ‚Äúa cat wearing a hat in Van Gogh style‚Äù) is encoded using:\n",
    "\n",
    "  * **CLIP** (used in DALL¬∑E 2, Stable Diffusion)\n",
    "  * **T5 or BERT** (used in Imagen)\n",
    "\n",
    "These embeddings are then:\n",
    "\n",
    "* Added to model input\n",
    "* Injected via **cross-attention**, **classifier-free guidance**, or **concatenation**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Sampling with Guidance (Classifier-Free Guidance)**\n",
    "\n",
    "To steer the model strongly toward the text prompt:\n",
    "\n",
    "$$\n",
    "\\epsilon_{\\text{guided}} = (1 + w)\\epsilon_\\theta(x_t, t, c) - w \\epsilon_\\theta(x_t, t, \\varnothing)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* `c`: the actual text prompt\n",
    "* `‚àÖ`: null/empty conditioning\n",
    "* `w`: guidance weight (e.g., 3.0)\n",
    "\n",
    "This **amplifies alignment** with the text, while maintaining sample quality.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Text-to-Image In Practice**\n",
    "\n",
    "| Model                | Conditioning                               |\n",
    "| -------------------- | ------------------------------------------ |\n",
    "| **DALL¬∑E 2**         | CLIP embedding + Prior + Diffusion decoder |\n",
    "| **Imagen (Google)**  | T5 embedding + UNet diffusion              |\n",
    "| **Stable Diffusion** | CLIP text embedding + Latent Diffusion     |\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Example Architecture: Stable Diffusion\n",
    "\n",
    "* Input: pure noise in **latent space**\n",
    "* Text encoded via CLIP\n",
    "* Conditioning is added in UNet layers (cross-attention)\n",
    "* Denoising loop is guided by prompt\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why DDPMs Work Well for Text-to-Image\n",
    "\n",
    "| Property              | Benefit                                          |\n",
    "| --------------------- | ------------------------------------------------ |\n",
    "| High sample diversity | Matches creativity of open-ended prompts         |\n",
    "| Stable training       | Unlike GANs ‚Äî less mode collapse                 |\n",
    "| Flexible conditioning | Text, layout, sketch, segmentation, etc.         |\n",
    "| Guidance tricks       | Classifier-free guidance enables controllability |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "| Step         | What's Happening                                    |\n",
    "| ------------ | --------------------------------------------------- |\n",
    "| Training     | Learn to denoise images **given a text prompt**     |\n",
    "| Conditioning | Text ‚Üí embedding (CLIP, T5) ‚Üí input to UNet         |\n",
    "| Sampling     | Reverse diffusion from noise ‚Üí image guided by text |\n",
    "| Output       | Realistic, diverse image matching the prompt        |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a **code-level breakdown of how conditioning is injected** into the UNet or how classifier-free guidance works during inference!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee26d9fa",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c38c67",
   "metadata": {},
   "source": [
    "### In the model training, is it ideal to keep the T very high? If not then what would be the problem if we do that?\n",
    "\n",
    "Great question ‚Äî you're asking about the role of the total number of diffusion steps `T` during training of denoising diffusion models.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Short Answer:\n",
    "\n",
    "> **No, it's not ideal to keep `T` very high**.\n",
    "> There's a trade-off. Setting `T` too high **slows down training and sampling**, and **can actually hurt performance**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Let‚Äôs unpack this in detail:\n",
    "\n",
    "---\n",
    "\n",
    "### üî¢ What is `T`?\n",
    "\n",
    "* `T` = total number of diffusion steps (e.g., 1000 in the original DDPM paper).\n",
    "* Forward process: Adds tiny Gaussian noise step-by-step from `x‚ÇÄ` to `x_T ~ N(0, I)`.\n",
    "* Reverse process: Model denoises `x_T ‚Üí x_{T-1} ‚Üí ... ‚Üí x_0`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è What happens when you set `T` very high?\n",
    "\n",
    "### 1. **Training becomes harder**\n",
    "\n",
    "* Each denoising step learns to reverse a **very small amount of noise**.\n",
    "* This makes the **signal-to-noise ratio** low per step.\n",
    "* The model needs to learn **many nearly-identical tasks**, which can be inefficient.\n",
    "\n",
    "> Too many steps = slow learning per step\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Sampling becomes extremely slow**\n",
    "\n",
    "* You need to run the model **T times** during inference.\n",
    "* If T = 1000, you call the model 1000 times just to sample 1 image!\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Diminishing returns**\n",
    "\n",
    "* After a certain T (e.g., 1000), increasing it further doesn't improve image quality significantly.\n",
    "* The gains plateau, but cost increases linearly.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Numerical instability**\n",
    "\n",
    "* With many small steps, **floating-point precision** issues may accumulate, especially in the reverse process.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What‚Äôs a good `T`?\n",
    "\n",
    "| Model                          | T used                                          |\n",
    "| ------------------------------ | ----------------------------------------------- |\n",
    "| Original DDPM (Ho et al. 2020) | 1000                                            |\n",
    "| DDIM (Improved sampling)       | 50‚Äì100                                          |\n",
    "| Stable Diffusion               | 50 steps during inference (trained with T=1000) |\n",
    "| Imagen                         | 1000                                            |\n",
    "\n",
    "So:\n",
    "\n",
    "* **Train with T = 1000** (still standard)\n",
    "* **Sample with fewer steps** (e.g., 50‚Äì250) using **DDIM** or **PLMS**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Solution: Faster sampling with same T\n",
    "\n",
    "Use methods like:\n",
    "\n",
    "* **DDIM** (Denoising Diffusion Implicit Models)\n",
    "* **FastSampler / DDPM++**\n",
    "* **Stochastic Samplers (DPM-Solver)**\n",
    "\n",
    "These can **simulate the full T-step reverse process in fewer steps**, with little or no quality drop.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Summary\n",
    "\n",
    "| T Value                 | Pros                      | Cons                                       |\n",
    "| ----------------------- | ------------------------- | ------------------------------------------ |\n",
    "| Small T (e.g., 100‚Äì200) | Fast training/sampling    | May degrade sample quality                 |\n",
    "| Medium T (e.g., 1000)   | Balanced performance      | Standard in practice                       |\n",
    "| Very High T (>1000)     | Very fine-grained control | Slow, redundant, possible numerical issues |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you want a visual showing noise levels vs timesteps or how sampling speed improves with DDIM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a10f5",
   "metadata": {},
   "source": [
    "## Gotcha! Thanks for the clarification. Now tell me, with this sample_timestep method can I skip any t from my T span, while inferencing from a trained model?\n",
    "\n",
    "Great follow-up ‚Äî and **yes**, you **can skip timesteps** during inference using a trained diffusion model, but there are **important trade-offs** and **specific methods** to do it correctly.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Default Behavior (DDPM)\n",
    "\n",
    "In standard DDPM inference, you start from $x_T \\sim \\mathcal{N}(0, I)$ and iteratively denoise using all $T$ steps down to $x_0$. So for $T = 300$, you do **300 reverse steps**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Can You Skip Steps?\n",
    "\n",
    "Yes, and here are the **main strategies**:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **DDIM (Denoising Diffusion Implicit Models)**\n",
    "\n",
    "DDIM allows you to:\n",
    "\n",
    "* **Choose a subset of timesteps**: e.g., use only 50 out of 300\n",
    "* **Make reverse steps deterministic**\n",
    "* Keep the **output distribution similar to DDPM**\n",
    "\n",
    "It introduces a **non-Markovian update rule** that enables you to skip timesteps without violating the model‚Äôs assumptions.\n",
    "\n",
    "### DDIM update rule (simplified):\n",
    "\n",
    "$$\n",
    "x_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}} x_0 + \\sqrt{1 - \\bar{\\alpha}_{t-1}} \\cdot \\epsilon\n",
    "$$\n",
    "\n",
    "But $x_0$ is computed via:\n",
    "\n",
    "$$\n",
    "x_0 = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\cdot \\hat{\\epsilon}_\\theta(x_t, t)}{\\sqrt{\\bar{\\alpha}_t}}\n",
    "$$\n",
    "\n",
    "Then you can select a **schedule**:\n",
    "\n",
    "```python\n",
    "timesteps = np.linspace(0, T-1, num=N, dtype=int)  # N << T\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Fast Sampling Schedules (e.g., Karras, DPM-Solver)**\n",
    "\n",
    "Advanced samplers (like [DPM-Solver](https://arxiv.org/abs/2206.00927)):\n",
    "\n",
    "* Use **ODE solvers** to do **as few as 5‚Äì15 steps**\n",
    "* Are faster than DDIM\n",
    "* Still preserve image quality (better than DDIM at fewer steps)\n",
    "\n",
    "These are plug-and-play with trained DDPMs but require changing the sampler code.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Important Notes\n",
    "\n",
    "* You **can‚Äôt just randomly drop steps** unless the sampler is **designed** to do so.\n",
    "* DDPM reverse steps are **stochastic** and **Markovian** ‚Äî skipping breaks that.\n",
    "* **DDIM and DPM-Solvers** introduce math to make **non-uniform, non-sequential steps work**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ TL;DR\n",
    "\n",
    "| Method          | Can skip timesteps? | Type          | Typical # steps | Quality   |\n",
    "| --------------- | ------------------- | ------------- | --------------- | --------- |\n",
    "| DDPM            | ‚ùå No (must use all) | Stochastic    | 100‚Äì1000        | High      |\n",
    "| DDIM            | ‚úÖ Yes               | Deterministic | 25‚Äì100          | High      |\n",
    "| DPM-Solver      | ‚úÖ Yes               | Deterministic | 5‚Äì20            | Very High |\n",
    "| Random skipping | ‚ùå Not recommended   | ‚ùå             | ‚ùå               | ‚ùå         |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like help converting your existing DDPM inference to a DDIM-style or faster scheduler?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecb7a07",
   "metadata": {},
   "source": [
    "## Few chat back you suggested one method with code. Option 1: Subsampled DDPM Sampling. Can't that be applied?\n",
    "\n",
    "Yes ‚Äî great catch! The **\"Subsampled DDPM Sampling\"** method I mentioned earlier *can* be applied ‚Äî but **with caveats**. Let‚Äôs break this down precisely so you understand **when it works**, **how it compares to DDIM**, and **how to use it correctly**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Recap: Subsampled DDPM Sampling\n",
    "\n",
    "This refers to **running the trained DDPM reverse process, but only on a subset of timesteps** (e.g., instead of 300, use 50 selected timesteps).\n",
    "\n",
    "### üîß Code pattern:\n",
    "\n",
    "```python\n",
    "timesteps = np.linspace(0, T-1, num=50, dtype=int)[::-1]\n",
    "x_t = torch.randn_like(image_shape)\n",
    "for t in timesteps:\n",
    "    x_t = model_p_sample(x_t, t)\n",
    "```\n",
    "\n",
    "This is **simple to implement** and speeds up sampling.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è But Here‚Äôs the Catch\n",
    "\n",
    "### ‚ùå DDPM ‚â† Designed to Skip Steps\n",
    "\n",
    "In DDPM, the **reverse process is Markovian and stochastic**, and each reverse step is trained assuming the **exact previous step existed**.\n",
    "\n",
    "So skipping steps:\n",
    "\n",
    "* **Breaks assumptions**\n",
    "* Introduces **sampling artifacts** (blur, loss of details)\n",
    "* May not preserve fidelity\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why It *Sometimes Works*\n",
    "\n",
    "* The denoising process is often **robust enough** that small skips (e.g., sampling every 3rd or 6th step) don‚Äôt break the image completely.\n",
    "* Some models (esp. those trained with higher noise schedules or with classifier-free guidance) generalize surprisingly well to subsampling.\n",
    "\n",
    "But it‚Äôs still **a hack**, not a principled method.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ How It's Different From DDIM\n",
    "\n",
    "| Aspect             | Subsampled DDPM    | DDIM                      |\n",
    "| ------------------ | ------------------ | ------------------------- |\n",
    "| Backed by theory?  | ‚ùå No               | ‚úÖ Yes (ODE formulation)   |\n",
    "| Deterministic?     | ‚ùå No               | ‚úÖ Yes (optionally)        |\n",
    "| Sampling quality?  | ‚ö†Ô∏è Often degraded  | ‚úÖ Often preserved         |\n",
    "| Speed              | ‚úÖ Faster           | ‚úÖ Faster                  |\n",
    "| Need model change? | ‚ùå No change needed | ‚ùå No (just sampler logic) |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ When to Use Subsampled DDPM\n",
    "\n",
    "You **can** use subsampled DDPM when:\n",
    "\n",
    "* You want faster sampling without major code changes\n",
    "* You're okay with **slightly lower visual quality**\n",
    "* You don't have time to implement DDIM or DPM-Solver\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Safer Alternative\n",
    "\n",
    "If you're okay to make small code edits, **DDIM** is always the better route for:\n",
    "\n",
    "* Skipping steps properly\n",
    "* Retaining high fidelity\n",
    "* Going fully deterministic (if needed)\n",
    "\n",
    "---\n",
    "\n",
    "## Would You Like a Code Snippet?\n",
    "\n",
    "Let me know if you want:\n",
    "\n",
    "* A simple implementation of subsampled DDPM\n",
    "* A working DDIM sampler (with your model)\n",
    "* A visual comparison between DDPM full / DDPM subsampled / DDIM\n",
    "\n",
    "All are doable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0cae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
