{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae3f798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ytiam\\notebooks\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ytiam\\notebooks\\.venv\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find cuobjdump.exe\n",
      "  warnings.warn(f\"Failed to find {binary}\")\n",
      "c:\\Users\\ytiam\\notebooks\\.venv\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find nvdisasm.exe\n",
      "  warnings.warn(f\"Failed to find {binary}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stage6_end_to_end.py\n",
    "from langgraph.graph import StateGraph, MessagesState, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\n",
    "import wikipedia\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Annotated\n",
    "from langgraph.graph.message import add_messages # Import add_messages\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "load_dotenv(\"../../../config/local.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "672a1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator # Import the operator module\n",
    "\n",
    "class CustomMessagesState(MessagesState):\n",
    "    draft: str\n",
    "    feedback: str\n",
    "    score: int\n",
    "    task: str\n",
    "    revise_iter: int\n",
    "    subtasks: str\n",
    "    subtask_index: int\n",
    "    research: Annotated[List[Dict], operator.add]\n",
    "    next_node: str\n",
    "    tool_args: str\n",
    "    last_research_msg: str\n",
    "    error: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c85feb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- Config ----------\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "EVAL_THRESHOLD = 8\n",
    "MAX_REVISE_ITER = 2\n",
    "CHECKPOINT_FILE = \"agent_checkpoints.json\"\n",
    "\n",
    "# ---------- Helpers: simple file memory ----------\n",
    "def load_checkpoints():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_checkpoint(key, value):\n",
    "    data = load_checkpoints()\n",
    "    data[key] = value\n",
    "    with open(CHECKPOINT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9454c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Tools ----------\n",
    "\n",
    "@tool\n",
    "def wiki_search(query: str) -> str:\n",
    "    \"\"\"Search Wikipedia for a summary on the given query.\"\"\"\n",
    "    try:\n",
    "        return wikipedia.summary(query, sentences=2)\n",
    "    except Exception:\n",
    "        return \"No summary found.\"\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "@tool\n",
    "def read_csv_tool(input_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Read a CSV file from a string or base64-encoded bytes and return its contents.\n",
    "    Input format:\n",
    "    - For text CSV: pass CSV text directly\n",
    "    - For file bytes: pass base64 string starting with 'base64:'\n",
    "    Returns:\n",
    "        CSV content preview + column names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"REACHED read_csv_tool\")\n",
    "        if input_str.startswith(\"base64:\"):\n",
    "            import base64\n",
    "            bdata = base64.b64decode(input_str.replace(\"base64:\", \"\"))\n",
    "            df = pd.read_csv(io.BytesIO(bdata))\n",
    "        else:\n",
    "            df = pd.read_csv(io.StringIO(input_str))\n",
    "\n",
    "        preview = df.head().to_string()\n",
    "        cols = \", \".join(df.columns)\n",
    "\n",
    "        return f\"Columns: {cols}\\n\\nPreview:\\n{preview}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error reading CSV: {str(e)}\"\n",
    "\n",
    "\n",
    "TOOLS = [wiki_search, calculator, read_csv_tool]\n",
    "\n",
    "# ---------- LLM ----------\n",
    "llm = ChatOpenAI(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15ff5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_numerator(feedback: str, default: int = 7) -> int:\n",
    "    \"\"\"\n",
    "    Extracts the first integer before a slash (/) in the given feedback string.\n",
    "    Returns `default` if no such pattern is found.\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\b(\\d+)(?=\\s*/)', feedback)\n",
    "    return int(match.group(1)) if match else default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7169124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- System prompts ----------\n",
    "SYSTEM_PLANNER = SystemMessage(content=(\"You are a planner that breaks a high-level task into subtasks.\"\n",
    "    \"Understand the user's main task and create clear, manageable subtasks to achieve it.\"\n",
    "    \"If the task involves CSV data, ensure at least one subtask addresses it. and if required include the data context from the user provided context\"\n",
    "))\n",
    "SYSTEM_RESEARCH = SystemMessage(content=(\n",
    "    \"You are a researcher that decides which tool to call. \"\n",
    "    \"Return an LLM response that may include a tool_call if you need external data.\"\n",
    "    \"If the user provides a CSV or requests analysis of CSV data, you MUST call the 'read_csv_tool' tool.\"\n",
    "))\n",
    "SYSTEM_WRITER = SystemMessage(content=\"You are a writer. Synthesize the provided research into a concise, clear answer.\")\n",
    "SYSTEM_EVALUATOR = SystemMessage(content=\"You are an evaluator. Score from 1-10 and provide short critique and improvement points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f74c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Node implementations ----------\n",
    "def planner_node(state: CustomMessagesState):\n",
    "    try:\n",
    "        user_task = state[\"task\"]\n",
    "        # simple decomposition prompt\n",
    "        prompt = f\"Task: {user_task}\\nBreak this into 2 short subtasks (one sentence each). Make the subtasks pointed, coincise and task oriented to cover the task requirements. \\\n",
    "                you only have access to tools like wiki_search, calculator and read_csv_tool. if the task involves csv data, ensure one subtask is about reading and understanding the csv data provided.\"\n",
    "        resp = llm.invoke([SYSTEM_PLANNER, HumanMessage(content=prompt)])\n",
    "        subtasks = [s.strip() for s in resp.content.split(\"\\n\") if s.strip()]\n",
    "        # fallback if LLM didn't give 3 lines\n",
    "        if len(subtasks) < 1:\n",
    "            subtasks = [user_task]\n",
    "        print(\"Planner generated subtasks:\", subtasks)\n",
    "        return {\"subtasks\": subtasks, \"subtask_index\": 0, \"research\": []}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "def researcher_node(state: CustomMessagesState):\n",
    "    try:\n",
    "        idx = state[\"subtask_index\"]\n",
    "        subtasks: List[str] = state[\"subtasks\"]\n",
    "        if idx >= len(subtasks):\n",
    "            # nothing left\n",
    "            return {} #{\"done\": True}\n",
    "        current = subtasks[idx]\n",
    "        # create messages including system and brief history\n",
    "        # messages = [SYSTEM_RESEARCH, HumanMessage(content=f\"Research this: {current}\")]\n",
    "\n",
    "        messages = [\n",
    "            SYSTEM_RESEARCH,\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Research this subtask:\\n{current}\\n\\n\"\n",
    "                    f\"Original user data (for reference):\\n{state.get('task','')}\"\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # allow model to call tools\n",
    "        response = llm.bind_tools(TOOLS).invoke(messages)\n",
    "        # If tool call -> handle via tool node (route will be to 'tool')\n",
    "        if response.tool_calls:\n",
    "            # carry needed context for tool node\n",
    "            return {\n",
    "                \"research\": state.get(\"research\", []),\n",
    "                \"subtasks\": subtasks,\n",
    "                \"subtask_index\": idx,\n",
    "                \"next_node\": response.tool_calls[0][\"name\"],\n",
    "                \"tool_args\": response.tool_calls[0][\"args\"],\n",
    "                \"last_research_msg\": response  # store the model message that requested the tool\n",
    "            }\n",
    "\n",
    "        # If no tool call, treat response as observation/result\n",
    "        observations = state.get(\"research\", []) + [{\"subtask\": current, \"result\": response.content}]\n",
    "        return {\n",
    "            \"research\": observations,\n",
    "            \"subtasks\": subtasks,\n",
    "            \"subtask_index\": idx + 1\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "def tool_node(state: CustomMessagesState):\n",
    "    try:\n",
    "        subtasks = state.get(\"subtasks\", [])\n",
    "        idx = state.get(\"subtask_index\", 0)\n",
    "\n",
    "        # Safety check\n",
    "        if not subtasks or idx >= len(subtasks):\n",
    "            print(\"[tool_node] No valid subtask found, skipping tool execution.\")\n",
    "            #return {\"done\": True}\n",
    "            return {\n",
    "                \"research\": state.get(\"research\", []),\n",
    "                \"subtasks\": subtasks,\n",
    "                \"subtask_index\": idx,\n",
    "                \"next_node\": None,\n",
    "            }\n",
    "        tool_name = state[\"next_node\"]\n",
    "        tool_args = state[\"tool_args\"]\n",
    "        last_model_msg = state[\"last_research_msg\"]\n",
    "        # execute appropriate tool\n",
    "        result = None\n",
    "        for t in TOOLS:\n",
    "            if t.name == tool_name:\n",
    "                result = t.invoke(tool_args)\n",
    "                break\n",
    "        if result is None: \n",
    "            result = f\"Tool {tool_name} not found.\"\n",
    "\n",
    "        # Build a ToolMessage linked to the tool_call_id from the stored model message\n",
    "        tool_call_id = None\n",
    "        if hasattr(last_model_msg, \"tool_calls\") and last_model_msg.tool_calls:\n",
    "            tool_call_id = last_model_msg.tool_calls[0][\"id\"]\n",
    "        # if no id, generate a safe fallback\n",
    "        if not tool_call_id:\n",
    "            tool_call_id = \"auto_tool_call\"\n",
    "\n",
    "        tool_msg = ToolMessage(content=result, tool_call_id=tool_call_id)\n",
    "        # append tool observation into research history\n",
    "        observations = state.get(\"research\", []) + [{\"subtask\": state[\"subtasks\"][state[\"subtask_index\"]], \"result\": result}]\n",
    "        return {\n",
    "            \"research\": observations,\n",
    "            \"subtasks\": state[\"subtasks\"],\n",
    "            \"subtask_index\": state[\"subtask_index\"] + 1,  # move to next subtask after tool result\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "def writer_node(state: CustomMessagesState):\n",
    "    try:\n",
    "        print(\"Reached writer node\")\n",
    "        # gather all research into a single prompt\n",
    "        research = state.get(\"research\", [])\n",
    "        topic = state.get(\"task\")\n",
    "        research_text = \"\\n\".join([f\"- {r['subtask']}: {r['result']}\" for r in research])\n",
    "        prompt = f\"Topic: {topic}\\nResearch gathered:\\n{research_text}\\n\\nWrite final answer (concise).\"\n",
    "        resp = llm.invoke([SYSTEM_WRITER, HumanMessage(content=prompt)])\n",
    "        return {\"draft\": resp.content, \"research\": research, \"task\": topic, \"revise_iter\": 0}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def evaluator_node(state: CustomMessagesState):\n",
    "    try:\n",
    "        print(\"Reached Eval Node\")\n",
    "        draft = state[\"draft\"]\n",
    "        prompt = f\"Text:\\n{draft}\\n\\nGive a score from 1–10 in the format `score: x/10`, then give a 1-line critique and 1 improvement suggestion.\"\n",
    "        resp = llm.invoke([SYSTEM_EVALUATOR, HumanMessage(content=prompt)])\n",
    "        # crude score parsing\n",
    "        score = extract_numerator(resp.content, default=7)\n",
    "        print(f\"Evaluation score: {score}, feedback: {resp.content}\")\n",
    "        return {\"draft\": draft, \"feedback\": resp.content, \"score\": score, \"revise_iter\": state.get(\"revise_iter\", 0)}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluator_node: {str(e)}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "def reviser_node(state: CustomMessagesState):\n",
    "    try:\n",
    "        draft = state[\"draft\"]\n",
    "        feedback = state[\"feedback\"]\n",
    "        iter_count = state.get(\"revise_iter\", 0) + 1\n",
    "        prompt = f\"Improve the draft based on this feedback:\\nFeedback: {feedback}\\n\\nDraft:\\n{draft}\"\n",
    "        resp = llm.invoke([HumanMessage(content=prompt)])\n",
    "        return {\"draft\": resp.content, \"revise_iter\": iter_count, \"feedback\": state[\"feedback\"], \"score\": state[\"score\"], \"task\": state.get(\"task\")}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def error_node(state: CustomMessagesState):\n",
    "    return {\"draft\": f\"Agent failed safely. Error: {state.get('error')}\"}\n",
    "\n",
    "def route_researcher(state: CustomMessagesState):\n",
    "    if (\"next_node\" in state) and (state[\"next_node\"] is not None):\n",
    "        return \"tool\"\n",
    "    \n",
    "    idx = state.get(\"subtask_index\", 0)\n",
    "    subtasks = state.get(\"subtasks\", [])\n",
    "    \n",
    "    # Check if there are still subtasks left to process\n",
    "    if idx < len(subtasks):\n",
    "        return \"researcher\"\n",
    "    else:\n",
    "        # All subtasks complete\n",
    "        return \"writer\"\n",
    "    \n",
    "\n",
    "def route_eval(state:CustomMessagesState):\n",
    "    score = state.get(\"score\", 0)\n",
    "    revise_iter = state.get(\"revise_iter\", 0)\n",
    "    if score < EVAL_THRESHOLD and revise_iter < MAX_REVISE_ITER:\n",
    "        return \"reviser\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "def route_errors(state):\n",
    "    if \"error\" in state:\n",
    "        return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-----------------------------------------------\n",
    "\n",
    "builder1 = StateGraph(CustomMessagesState)\n",
    "\n",
    "builder1.add_node(\"planner\", planner_node)\n",
    "builder1.add_node(\"researcher\", researcher_node)\n",
    "builder1.add_node(\"tool\", tool_node)\n",
    "builder1.add_node(\"writer\", writer_node)\n",
    "builder1.add_node(\"evaluator\", evaluator_node)\n",
    "builder1.add_node(\"reviser\", reviser_node)\n",
    "# builder1.add_node(\"error\", error_node)\n",
    "\n",
    "builder1.set_entry_point(\"planner\")\n",
    "builder1.add_edge(\"planner\", \"researcher\")\n",
    "\n",
    "builder1.add_conditional_edges(\"researcher\", route_researcher, {\"tool\": \"tool\", \"writer\": \"writer\"})\n",
    "# builder1.add_conditional_edges(\"tool\", route_tool, {\"researcher\": \"researcher\", END: END})\n",
    "builder1.add_edge(\"tool\", \"researcher\")\n",
    "\n",
    "builder1.add_edge(\"writer\", \"evaluator\")\n",
    "builder1.add_conditional_edges(\"evaluator\", route_eval, {\"reviser\": \"reviser\", END: END})\n",
    "builder1.add_edge(\"reviser\", \"evaluator\")\n",
    "\n",
    "# builder1.add_edge(\"planner\", \"error\")\n",
    "# builder1.add_edge(\"researcher\", \"error\")\n",
    "# builder1.add_edge(\"tool\", \"error\")\n",
    "# builder1.add_edge(\"writer\", \"error\")\n",
    "\n",
    "graph1 = builder1.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecce8494",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_recursion_limit = 25\n",
    "\n",
    "# Create a RunnableConfig object with the specified recursion limit\n",
    "config = RunnableConfig(recursion_limit=custom_recursion_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aa41ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph1\n",
    "# prompt1 -> \"Write a detailed analysis of the following CSV data:\\n\\nName,Age,Occupation,Salary\\nAlice,30,Engineer,70000\\nBob,25,Designer,50000\\n;Diana,28,Doctor,80000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74a5ba0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planner generated subtasks: [\"1. Use the read_csv_tool to analyze the provided CSV file and extract relevant data about individuals' ages and occupations, which can help illustrate points in explaining the theory of relativity.\", '2. Research the theory of relativity using wiki_search and compile a concise explanation that incorporates insights from the CSV data.']\n",
      "REACHED read_csv_tool\n",
      "REACHED read_csv_tool\n",
      "[tool_node] No valid subtask found, skipping tool execution.\n",
      "Reached writer node\n",
      "Reached Eval Node\n",
      "Evaluation score: 8, feedback: score: 8/10  \n",
      "Critique: The explanation of relativity is clear and well-structured, but the connection to the CSV data feels somewhat forced.  \n",
      "Improvement suggestion: Strengthen the link between relativity and the data by incorporating a more direct analysis or visualization of how different scenarios could illustrate the concepts presented.\n"
     ]
    }
   ],
   "source": [
    "init_state = {\"task\": \"Explain the theory of relativity using data from the provided CSV file:\\n\\nName,Age,Occupation,Salary\\nAlice,30,Engineer,70000\\nBob,25,Designer,50000\\nDiana,28,Doctor,80000\"}\n",
    "result = graph1.invoke(init_state, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "696ad4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The theory of relativity, proposed by Albert Einstein, encompasses two main concepts: special relativity and general relativity. Special relativity addresses the physics of objects moving at constant speeds, particularly close to the speed of light, and introduces the principle that the laws of physics are the same for all observers, regardless of their relative motion. It also highlights the interdependence of time and space, stating that time can dilate and lengths can contract based on an observer's speed.\n",
      "\n",
      "General relativity extends these ideas to include gravity as a curvature of spacetime caused by mass. Essentially, massive objects like planets and stars warp the fabric of spacetime around them, affecting the motion of other objects.\n",
      "\n",
      "To illustrate these concepts using the provided CSV data, let's consider the ages and occupations of individuals. For instance, Alice (30), Bob (25), and Diana (28), while different ages, all perceive time similarly in their daily lives as they go about their professions (Engineer, Designer, Doctor). However, if they were to travel at significant fractions of the speed of light, their experiences of time would diverge—Alice might age more slowly compared to Bob and Diana, demonstrating the relativity of time as affected by speed, a key aspect of the theory.\n",
      "\n",
      "In conclusion, the theory of relativity fundamentally reshapes our understanding of time and space, illustrating how they are relative to the observer's frame of reference.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"draft\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3094a29e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
