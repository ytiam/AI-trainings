{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2547f5",
   "metadata": {},
   "source": [
    "**Let‚Äôs move to Stage 2: Tool-Augmented Agent ‚Äî where your agent goes from just talking to thinking, acting, and observing like a true reasoning system.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9087204f",
   "metadata": {},
   "source": [
    "üß© Stage 2: Tool-Augmented Agent (Reason ‚Üí Act ‚Üí Observe)\n",
    "\n",
    "üéØ Goal\n",
    "\n",
    "Teach your LangGraph agent to:\n",
    "\n",
    "- Call tools (like a calculator or Wikipedia search)\n",
    "\n",
    "- Parse tool calls from LLM output\n",
    "\n",
    "- Feed tool results back into the reasoning loop\n",
    "\n",
    "This introduces the core agentic loop:\n",
    "üëâ Reason ‚Üí Act ‚Üí Observe ‚Üí Reflect\n",
    "\n",
    "üß† Concepts Introduced\n",
    "\n",
    "- Tool nodes: Represent capabilities (APIs, functions, etc.)\n",
    "\n",
    "- Structured output (tool_calls) from LLM messages\n",
    "\n",
    "- Dynamic routing: Deciding at runtime which node to execute next\n",
    "\n",
    "- Observation updates: Feeding tool results back into the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881e6ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ytiam\\notebooks\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState, END, START\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "import wikipedia\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../../config/local.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff3c3a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(MessagesState):\n",
    "    next_node: str\n",
    "    tool_args: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "392b786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1Ô∏è‚É£ Define tools\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Evaluates a simple arithmetic expression.\"\"\"\n",
    "    print(f\"Calculating: {expression}\")\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "@tool\n",
    "def wiki_search(query: str) -> str:\n",
    "    \"\"\"Searches Wikipedia and returns a short summary.\"\"\"\n",
    "    print(f\"Searching Wikipedia for: {query}\")\n",
    "    try:\n",
    "        return wikipedia.summary(query, sentences=2)\n",
    "    except Exception:\n",
    "        return \"No summary found.\"\n",
    "\n",
    "tools = [calculator, wiki_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa96b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2Ô∏è‚É£ Define the LLM (tool-aware model)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61b996ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage, SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac0797b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode # Pre-built node for executing tools\n",
    "\n",
    "# Create a ToolNode instance. This node will automatically execute any tool calls \n",
    "# generated by an LLM that is bound to these tools.\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef6c7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "165f5916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3Ô∏è‚É£ Agent node ‚Äî decides what to do next\n",
    "def agent_node(state: State):\n",
    "    system_prompt = SystemMessage(\n",
    "    content=(\n",
    "        \"You are a helpful assistant. \"\n",
    "        \"You can use tools when needed to answer the user's question. \"\n",
    "        \"If you already have all the information you need, respond directly. \"\n",
    "        \"When your answer is complete, do NOT call any more tools.\\n\\n\"\n",
    "        \"Below is the user query.\\n\"\n",
    "\n",
    "    )\n",
    ")\n",
    "    messages = [system_prompt] + state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "\n",
    "    return {\"messages\": messages + [response]}\n",
    "\n",
    "\n",
    "def should_continue(state: State, config: RunnableConfig):\n",
    "    # Get the list of messages from the current state.\n",
    "    messages = state[\"messages\"]\n",
    "    # Get the last message, which is the response from the agent.\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    if not last_message.tool_calls:\n",
    "        # If there are no tool calls, we are done. So, we\n",
    "        return \"end\"\n",
    "    # Otherwise, if there are tool calls,\n",
    "    else:\n",
    "        # We need to execute the tool(s). So, we return \"continue\" to route to the tool execution node.\n",
    "        return \"continue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb50834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5Ô∏è‚É£ Build graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"agent\", agent_node)\n",
    "builder.add_node(\"tool\", tool_node)\n",
    "\n",
    "builder.add_edge(START, \"agent\")\n",
    "builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        # If `should_continue` returns \"continue\", route to `music_tool_node`.\n",
    "        \"continue\": \"tool\",\n",
    "        # If `should_continue` returns \"end\", terminate the graph execution.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "builder.add_edge(\"tool\", \"agent\")\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a1fd116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching Wikipedia for: capital of France\n",
      "Calculating: 12 * 7\n"
     ]
    }
   ],
   "source": [
    "import uuid # Module for generating unique identifiers\n",
    "thread_id = uuid.uuid4()\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# 6Ô∏è‚É£ Run it\n",
    "query = \"What is the capital of France, and what is 12 * 7 and what is the age of Rahul Dravid as on 2025?\"\n",
    "result = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]}, config=config)\n",
    "\n",
    "# print(\"\\nü§ñ Final Answer:\", result[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "324823b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Final Answer: The capital of France is Paris. The result of \\( 12 \\times 7 \\) is 84.\n",
      "\n",
      "To determine the age of Rahul Dravid in 2025, we first note that he was born on January 11, 1973. So in 2025, he will turn 52 years old.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nü§ñ Final Answer:\", result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a30a58f",
   "metadata": {},
   "source": [
    "üß≠ How It Works\n",
    "\n",
    "- The agent node uses llm.bind_tools(tools) ‚Äî this allows the model to decide if it needs a tool call.\n",
    "\n",
    "- When a tool_call is detected, we route to the tool node.\n",
    "\n",
    "- The tool node executes the function and sends an observation back.\n",
    "\n",
    "- The graph loops back to the agent, which integrates the observation and produces the final answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715496f",
   "metadata": {},
   "source": [
    "‚úÖ What You Learned\n",
    "\n",
    "- How LangGraph enables tool integration through conditional node routing\n",
    "\n",
    "- How the Reason‚ÄìAct‚ÄìObserve loop is implemented structurally\n",
    "\n",
    "- The separation of thinking (agent) and doing (tool) phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb126c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
