{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8736bbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../../config/local.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9df7b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_content = \"\"\"Paul Graham's essay \"Founder Mode,\" published in September 2024, challenges conventional wisdom about scaling startups, arguing that founders should maintain their unique management style rather than adopting traditional corporate practices as their companies grow.\n",
    "Conventional Wisdom vs. Founder Mode\n",
    "The essay argues that the traditional advice given to growing companies—hiring good people and giving them autonomy—often fails when applied to startups.\n",
    "This approach, suitable for established companies, can be detrimental to startups where the founder's vision and direct involvement are crucial. \"Founder Mode\" is presented as an emerging paradigm that is not yet fully understood or documented, contrasting with the conventional \"manager mode\" often advised by business schools and professional managers.\n",
    "Unique Founder Abilities\n",
    "Founders possess unique insights and abilities that professional managers do not, primarily because they have a deep understanding of their company's vision and culture.\n",
    "Graham suggests that founders should leverage these strengths rather than conform to traditional managerial practices. \"Founder Mode\" is an emerging paradigm that is not yet fully understood or documented, with Graham hoping that over time, it will become as well-understood as the traditional manager mode, allowing founders to maintain their unique approach even as their companies scale.\n",
    "Challenges of Scaling Startups\n",
    "As startups grow, there is a common belief that they must transition to a more structured managerial approach. However, many founders have found this transition problematic, as it often leads to a loss of the innovative and agile spirit that drove the startup's initial success.\n",
    "Brian Chesky, co-founder of Airbnb, shared his experience of being advised to run the company in a traditional managerial style, which led to poor outcomes. He eventually found success by adopting a different approach, influenced by how Steve Jobs managed Apple.\n",
    "Steve Jobs' Management Style\n",
    "Steve Jobs' management approach at Apple served as inspiration for Brian Chesky's \"Founder Mode\" at Airbnb. One notable practice was Jobs' annual retreat for the 100 most important people at Apple, regardless of their position on the organizational chart\n",
    ". This unconventional method allowed Jobs to maintain a startup-like environment even as Apple grew, fostering innovation and direct communication across hierarchical levels. Such practices emphasize the importance of founders staying deeply involved in their companies' operations, challenging the traditional notion of delegating responsibilities to professional managers as companies scale.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37e9ed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build Index\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dc5104c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OllamaEmbeddings(model='nomic-embed-text:v1.5', show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2bfd456",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [Document(page_content=sample_content, metadata={\"Title\": \"Paul Graham's Founder Mode Essay\", \"Source\": \"https://www.perplexity.ai/page/paul-graham-s-founder-mode-ess-t9TCyvkqRiyMQJWsHr0fnQ\"})]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc863a73",
   "metadata": {},
   "source": [
    "`RecursiveCharacterTextSplitter` in LangChain is a **chunking algorithm** that splits text into pieces of size ~`chunk_size`, while trying to **preserve natural boundaries** (paragraphs → lines → sentences/words → characters) as much as possible.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Core idea (why “recursive”?)\n",
    "\n",
    "It tries multiple separators **in order**, from “best boundary” to “worst boundary”:\n",
    "\n",
    "Typical default separators:\n",
    "\n",
    "```python\n",
    "[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "```\n",
    "\n",
    "Meaning:\n",
    "\n",
    "1. Split by **paragraphs** (`\\n\\n`)\n",
    "2. If still too large, split by **newlines** (`\\n`)\n",
    "3. If still too large, split by **spaces** (` `)\n",
    "4. If still too large, split by **characters** (`\"\"`) ✅ last resort\n",
    "\n",
    "That is the “recursive” part:\n",
    "**if a chunk is too big → split it again using the next separator.**\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ What does it output?\n",
    "\n",
    "It produces chunks with:\n",
    "\n",
    "* `chunk_size` (max size)\n",
    "* `chunk_overlap` (overlap between chunks so context is not lost)\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "```\n",
    "\n",
    "So each chunk is ~1000 characters, but last 200 chars repeat into the next.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Step-by-step example\n",
    "\n",
    "Suppose you have:\n",
    "\n",
    "```\n",
    "[Paragraph A: 1500 chars]\n",
    "\n",
    "[Paragraph B: 800 chars]\n",
    "```\n",
    "\n",
    "With `chunk_size=1000`:\n",
    "\n",
    "### Step 1: split by `\\n\\n`\n",
    "\n",
    "It gets:\n",
    "\n",
    "* Chunk candidate A (1500 chars) ❌ too big\n",
    "* Chunk candidate B (800 chars) ✅ fine\n",
    "\n",
    "### Step 2: A is too big → split A using next separator `\\n`\n",
    "\n",
    "Now paragraph A becomes smaller line blocks.\n",
    "If still too big…\n",
    "\n",
    "### Step 3: split using `\" \"` (word boundaries)\n",
    "\n",
    "If still too big…\n",
    "\n",
    "### Step 4: split using `\"\"` (character level)\n",
    "\n",
    "Guaranteed to fit.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ How chunk_overlap is applied\n",
    "\n",
    "After it forms chunks, it **slides a window**:\n",
    "\n",
    "Example:\n",
    "\n",
    "* chunk1 ends at position 1000\n",
    "* chunk2 starts at position `1000 - overlap`\n",
    "\n",
    "So overlap gives continuity like:\n",
    "\n",
    "```\n",
    "chunk1: [0..1000]\n",
    "chunk2: [800..1800]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Why it’s used so much in RAG\n",
    "\n",
    "Because it prevents ugly cuts like this:\n",
    "\n",
    "❌ bad split:\n",
    "\n",
    "```\n",
    "...the model was trained using transfo\n",
    "rmer architecture...\n",
    "```\n",
    "\n",
    "✅ good split:\n",
    "\n",
    "* prefers paragraph / line / sentence / word splits first\n",
    "\n",
    "This improves:\n",
    "\n",
    "* retrieval quality\n",
    "* embedding meaning\n",
    "* answer coherence\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Important practical details\n",
    "\n",
    "### 1) It splits by **characters**, not tokens\n",
    "\n",
    "So `chunk_size=1000` = 1000 characters (not tokens).\n",
    "\n",
    "For token-based splitting you’d use token splitters (like tiktoken based ones).\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Separator selection is “best effort”\n",
    "\n",
    "It will try to keep structure, but if your text has no separators (like JSON blobs), it will eventually go to character splitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 3) It also “merges” smaller pieces\n",
    "\n",
    "After splitting, it tries to **combine adjacent small parts** until it reaches chunk_size.\n",
    "\n",
    "So it’s not just “split everything blindly”.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ad66f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200, chunk_overlap=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "470232b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d18bc2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_splits)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0fbae812",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(doc_splits):\n",
    "    doc.metadata['chunk_id'] = i+1 ### adding chunk id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c0717a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2be7bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GeneratePropositions(BaseModel):\n",
    "    \"\"\"List of all the propositions in a given document\"\"\"\n",
    "\n",
    "    propositions: List[str] = Field(\n",
    "        description=\"List of propositions (factual, self-contained, and concise information)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bc80b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1490bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm= llm.with_structured_output(GeneratePropositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "71de1f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposition_examples = [\n",
    "    {\"document\": \n",
    "        \"In 1969, Neil Armstrong became the first person to walk on the Moon during the Apollo 11 mission.\", \n",
    "     \"propositions\": \n",
    "        \"['Neil Armstrong was an astronaut.', 'Neil Armstrong walked on the Moon in 1969.', 'Neil Armstrong was the first person to walk on the Moon.', 'Neil Armstrong walked on the Moon during the Apollo 11 mission.', 'The Apollo 11 mission occurred in 1969.']\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "948217cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_proposition_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{document}\"),\n",
    "        (\"ai\", \"{propositions}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2220b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt = example_proposition_prompt,\n",
    "    examples = proposition_examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "30ba207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "system = \"\"\"Please break down the following text into simple, self-contained propositions. Ensure that each proposition meets the following criteria:\n",
    "\n",
    "    1. Express a Single Fact: Each proposition should state one specific fact or claim.\n",
    "    2. Be Understandable Without Context: The proposition should be self-contained, meaning it can be understood without needing additional context.\n",
    "    3. Use Full Names, Not Pronouns: Avoid pronouns or ambiguous references; use full entity names.\n",
    "    4. Include Relevant Dates/Qualifiers: If applicable, include necessary dates, times, and qualifiers to make the fact precise.\n",
    "    5. Contain One Subject-Predicate Relationship: Focus on a single subject and its corresponding action or attribute, without conjunctions or multiple clauses.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{document}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "00addbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposition_generator = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6cc050b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "propositions = [] # Store all the propositions from the document\n",
    "\n",
    "for i in range(len(doc_splits)):\n",
    "    response = proposition_generator.invoke({\"document\": doc_splits[i].page_content}) # Creating proposition\n",
    "    for proposition in response.propositions:\n",
    "        propositions.append(Document(page_content=proposition, metadata={\"Title\": \"Paul Graham's Founder Mode Essay\", \"Source\": \"https://www.perplexity.ai/page/paul-graham-s-founder-mode-ess-t9TCyvkqRiyMQJWsHr0fnQ\", \"chunk_id\": i+1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e6000e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradePropositions(BaseModel):\n",
    "    \"\"\"Grade a given proposition on accuracy, clarity, completeness, and conciseness\"\"\"\n",
    "\n",
    "    accuracy: int = Field(\n",
    "        description=\"Rate from 1-10 based on how well the proposition reflects the original text.\"\n",
    "    )\n",
    "    \n",
    "    clarity: int = Field(\n",
    "        description=\"Rate from 1-10 based on how easy it is to understand the proposition without additional context.\"\n",
    "    )\n",
    "\n",
    "    completeness: int = Field(\n",
    "        description=\"Rate from 1-10 based on whether the proposition includes necessary details (e.g., dates, qualifiers).\"\n",
    "    )\n",
    "\n",
    "    conciseness: int = Field(\n",
    "        description=\"Rate from 1-10 based on whether the proposition is concise without losing important information.\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "structured_llm= llm.with_structured_output(GradePropositions)\n",
    "\n",
    "# Prompt\n",
    "evaluation_prompt_template = \"\"\"\n",
    "Please evaluate the following proposition based on the criteria below:\n",
    "- **Accuracy**: Rate from 1-10 based on how well the proposition reflects the original text.\n",
    "- **Clarity**: Rate from 1-10 based on how easy it is to understand the proposition without additional context.\n",
    "- **Completeness**: Rate from 1-10 based on whether the proposition includes necessary details (e.g., dates, qualifiers).\n",
    "- **Conciseness**: Rate from 1-10 based on whether the proposition is concise without losing important information.\n",
    "\n",
    "Example:\n",
    "Docs: In 1969, Neil Armstrong became the first person to walk on the Moon during the Apollo 11 mission.\n",
    "\n",
    "Propositons_1: Neil Armstrong was an astronaut.\n",
    "Evaluation_1: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Propositons_2: Neil Armstrong walked on the Moon in 1969.\n",
    "Evaluation_3: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Propositons_3: Neil Armstrong was the first person to walk on the Moon.\n",
    "Evaluation_3: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Propositons_4: Neil Armstrong walked on the Moon during the Apollo 11 mission.\n",
    "Evaluation_4: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Propositons_5: The Apollo 11 mission occurred in 1969.\n",
    "Evaluation_5: \"accuracy\": 10, \"clarity\": 10, \"completeness\": 10, \"conciseness\": 10\n",
    "\n",
    "Format:\n",
    "Proposition: \"{proposition}\"\n",
    "Original Text: \"{original_text}\"\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", evaluation_prompt_template),\n",
    "        (\"human\", \"{proposition}, {original_text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "proposition_evaluator = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "911d0bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Propostion: Paul Graham wrote an essay titled 'Founder Mode'. \n",
      " Scores: {'accuracy': 10, 'clarity': 10, 'completeness': 5, 'conciseness': 10}\n",
      "Fail\n",
      "2) Propostion: The essay 'Founder Mode' was published in September 2024. \n",
      " Scores: {'accuracy': 10, 'clarity': 10, 'completeness': 5, 'conciseness': 10}\n",
      "Fail\n",
      "39) Propostion: Steve Jobs managed Apple. \n",
      " Scores: {'accuracy': 10, 'clarity': 10, 'completeness': 5, 'conciseness': 10}\n",
      "Fail\n"
     ]
    }
   ],
   "source": [
    "# Define evaluation categories and thresholds\n",
    "evaluation_categories = [\"accuracy\", \"clarity\", \"completeness\", \"conciseness\"]\n",
    "thresholds = {\"accuracy\": 7, \"clarity\": 7, \"completeness\": 7, \"conciseness\": 7}\n",
    "\n",
    "# Function to evaluate proposition\n",
    "def evaluate_proposition(proposition, original_text):\n",
    "    response = proposition_evaluator.invoke({\"proposition\": proposition, \"original_text\": original_text})\n",
    "    \n",
    "    # Parse the response to extract scores\n",
    "    scores = {\"accuracy\": response.accuracy, \"clarity\": response.clarity, \"completeness\": response.completeness, \"conciseness\": response.conciseness}  # Implement function to extract scores from the LLM response\n",
    "    return scores\n",
    "\n",
    "# Check if the proposition passes the quality check\n",
    "def passes_quality_check(scores):\n",
    "    for category, score in scores.items():\n",
    "        if score < thresholds[category]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "evaluated_propositions = [] # Store all the propositions from the document\n",
    "\n",
    "# Loop through generated propositions and evaluate them\n",
    "for idx, proposition in enumerate(propositions):\n",
    "    scores = evaluate_proposition(proposition.page_content, doc_splits[proposition.metadata['chunk_id'] - 1].page_content)\n",
    "    if passes_quality_check(scores):\n",
    "        # Proposition passes quality check, keep it\n",
    "        evaluated_propositions.append(proposition)\n",
    "    else:\n",
    "        # Proposition fails, discard or flag for further review\n",
    "        print(f\"{idx+1}) Propostion: {proposition.page_content} \\n Scores: {scores}\")\n",
    "        print(\"Fail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bda1cc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 44/44 [00:02<00:00, 17.03it/s]\n"
     ]
    }
   ],
   "source": [
    "vectorstore_propositions = FAISS.from_documents(evaluated_propositions, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "440a3afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OllamaEmbeddings(base_url='http://localhost:11434', model='nomic-embed-text:v1.5', embed_instruction='passage: ', query_instruction='query: ', mirostat=None, mirostat_eta=None, mirostat_tau=None, num_ctx=None, num_gpu=None, num_thread=None, repeat_last_n=None, repeat_penalty=None, temperature=None, stop=None, tfs_z=None, top_k=None, top_p=None, show_progress=True, headers=None, model_kwargs=None)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_propositions.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b94a531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_propositions = vectorstore_propositions.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={'k': 4}, # number of documents to retrieve\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6c080fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='c9e75959-39c3-4773-bfbc-71f562c7f864', metadata={'Title': \"Paul Graham's Founder Mode Essay\", 'Source': 'https://www.perplexity.ai/page/paul-graham-s-founder-mode-ess-t9TCyvkqRiyMQJWsHr0fnQ', 'chunk_id': 3}, page_content='Brian Chesky was advised to run Airbnb in a traditional managerial style.'),\n",
       " Document(id='2b04ffb1-09ef-48a0-ab32-b7c1b82de0b0', metadata={'Title': \"Paul Graham's Founder Mode Essay\", 'Source': 'https://www.perplexity.ai/page/paul-graham-s-founder-mode-ess-t9TCyvkqRiyMQJWsHr0fnQ', 'chunk_id': 3}, page_content=\"Steve Jobs' management style influenced Brian Chesky's approach at Airbnb.\"),\n",
       " Document(id='1a989229-9a63-46f9-b62b-805d5d835c1c', metadata={'Title': \"Paul Graham's Founder Mode Essay\", 'Source': 'https://www.perplexity.ai/page/paul-graham-s-founder-mode-ess-t9TCyvkqRiyMQJWsHr0fnQ', 'chunk_id': 3}, page_content='Brian Chesky found success by adopting a different management approach.'),\n",
       " Document(id='3b3648cb-16c7-4423-8e11-cc52a44b18b8', metadata={'Title': \"Paul Graham's Founder Mode Essay\", 'Source': 'https://www.perplexity.ai/page/paul-graham-s-founder-mode-ess-t9TCyvkqRiyMQJWsHr0fnQ', 'chunk_id': 3}, page_content='Brian Chesky is a co-founder of Airbnb.')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who's management approach served as inspiartion for Brian Chesky's \\\"Founder Mode\\\" at Airbnb?\"\n",
    "res_proposition = retriever_propositions.invoke(query)\n",
    "res_proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "18cbd765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Content: Brian Chesky was advised to run Airbnb in a traditional managerial style. --- Chunk_id: 3\n",
      "2) Content: Steve Jobs' management style influenced Brian Chesky's approach at Airbnb. --- Chunk_id: 3\n",
      "3) Content: Brian Chesky found success by adopting a different management approach. --- Chunk_id: 3\n",
      "4) Content: Brian Chesky is a co-founder of Airbnb. --- Chunk_id: 3\n"
     ]
    }
   ],
   "source": [
    "for i, r in enumerate(res_proposition):\n",
    "    print(f\"{i+1}) Content: {r.page_content} --- Chunk_id: {r.metadata['chunk_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc8dd33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefe4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6043f953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cb49d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
