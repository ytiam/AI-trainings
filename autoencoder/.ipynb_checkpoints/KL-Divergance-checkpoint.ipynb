{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d575e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c08a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    \"\"\"Compute the Shannon entropy of a distribution.\n",
    "\n",
    "    The Shannon entropy is defined as follows\n",
    "    :math:`\\sum_x p(x_i) * \\log p(x_i)`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : np.ndarray\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> distribution = np.array([0.25, 0.25, 0.5])\n",
    "    >>> entropy(distribution)\n",
    "    1.5\n",
    "\n",
    "    \"\"\"\n",
    "    assert isinstance(p, np.ndarray), '`p` must be a numpy array'\n",
    "    assert np.isclose(np.sum(p), 1.), '`p` must be a probability distribution'\n",
    "\n",
    "    p = p[np.nonzero(p)]\n",
    "\n",
    "    return np.sum(-p * np.log2(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "928e3198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5709505944546684"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(np.array([0.3,0.4,0.3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22857732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    \"\"\"Compute the Kullback-Leibler divergence between two distributions.\n",
    "\n",
    "    The KL divergence is defined as\n",
    "    :math:`D_{KL}(p, q) = \\sum_x p(x_i) * (\\log p(x_i) - \\log q(x_i))`\n",
    "    which can be rewritten as\n",
    "    :math:`D_{KL}(p, q) = \\sum_x p(x_i) * \\log \\frac{p(x_i)}{q(x_i)}`\n",
    "    and is computationally more conventient.\n",
    "\n",
    "    Some interesting properties of the KL divergence:\n",
    "      - The KL divergence is always non-negative, i.e.\n",
    "        :math:`D_{KL}(p, q) \\geq 0`.\n",
    "\n",
    "      - The KL divergence is additive for independent distributions, i.e.\n",
    "        :math:`D_{KL}(P, Q) = D_{KL}(P_1, Q_1) + D_{KL}(P_2, Q_2)`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : np.ndarray\n",
    "    q : np.ndarray\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> p = np.array([0.7, 0.2, 0.05, 0.05])\n",
    "    >>> q = np.array([0.05, 0.05, 0.2, 0.7])\n",
    "    >>> kl_divergence(p, q)\n",
    "    2.77478069934\n",
    "\n",
    "    \"\"\"\n",
    "    assert isinstance(p, np.ndarray), '`p` must be a numpy array'\n",
    "    assert np.isclose(np.sum(p), 1.), '`p` must be a probability distribution'\n",
    "\n",
    "    assert isinstance(q, np.ndarray), '`q` must be a numpy array'\n",
    "    assert np.isclose(np.sum(q), 1.), '`q` must be a probability distribution'\n",
    "\n",
    "    # Define the zero masks for P and Q and ignore them during computation\n",
    "    q_mask, p_mask = q == 0, p == 0\n",
    "    # The implication `p => q` is equivalent to `not p or q`\n",
    "    assert all(~q_mask | p_mask), 'The KL divergence is defined iif Q(x)=0 implies P(x)=0'\n",
    "    p, q = p[~p_mask], q[~q_mask]\n",
    "\n",
    "    return np.sum(p * np.log2(p / q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9222005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence(np.array([0.3,0.4,0.3]), np.array([0.3,0.4,0.3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4d139",
   "metadata": {},
   "source": [
    "Suppose, one random variable X has 50 data points and another random variable Y has 52 data points. Can I calculate KL Divergence of these two distributions ?\n",
    "\n",
    "- To compute the **Kullback-Leibler (KL) divergence** between two distributions \\( P \\) and \\( Q \\), both distributions must be defined over the same probability space.\n",
    "\n",
    "For your case:\n",
    "\n",
    "1. You have two random variables \\( X \\) (50 points) and \\( Y \\) (52 points).\n",
    "2. If you want to compute KL divergence, you first need to estimate the probability distributions \\( P(X) \\) and \\( Q(Y) \\).\n",
    "\n",
    "### Steps to Check Feasibility:\n",
    "- **Ensure Same Support**: KL divergence is only meaningful if \\( P(x) > 0 \\) implies \\( Q(x) > 0 \\). That is, every value where \\( P \\) has probability mass should also exist in \\( Q \\). Otherwise, KL divergence is undefined.\n",
    "- **Estimate Probability Distributions**: Since you only have discrete samples, you can estimate probability mass functions (PMFs) using **histograms** or **kernel density estimation (KDE)**.\n",
    "- **Bin Alignment**: If using histograms, make sure both distributions use the same binning strategy.\n",
    "- **Compute KL Divergence**: Using the formula:\n",
    "  \n",
    "  $\n",
    "  D_{\\text{KL}}(P \\parallel Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}\n",
    "  $\n",
    "\n",
    "### Possible Issues:\n",
    "- **Different Sample Sizes**: Since \\( X \\) has 50 samples and \\( Y \\) has 52, you need to estimate probability distributions from these finite samples.\n",
    "- **Zero Probability Issue**: If \\( Q(x) = 0 \\) for any \\( x \\) where \\( P(x) > 0 \\), KL divergence becomes undefined. A common fix is **smoothing techniques** like adding a small constant (Laplace smoothing) to probability estimates.\n",
    "\n",
    "### Conclusion:\n",
    "Yes, you **can** calculate KL divergence, but you need to **estimate distributions carefully** and ensure the support of \\( P \\) is contained in \\( Q \\). If there's a mismatch, use smoothing to avoid undefined values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2934911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7903a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_kl_divergence(p, q):\n",
    "    \"\"\"The symmetric Kullback-Leibler divergence.\n",
    "\n",
    "    Kullback and Leibler themselves defined the symmetric divergence as\n",
    "    :math:`D_{KL}(p, q) + D_{KL}(q, p)`.\n",
    "\n",
    "    \"\"\"\n",
    "    return kl_divergence(p, q) + kl_divergence(q, p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
