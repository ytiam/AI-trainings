{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94ed7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial: https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319cb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a78dc",
   "metadata": {},
   "source": [
    "In Transformers, the dot product is called **attention** because it is the mechanism that determines how much importance (or attention) should be given to other elements in a sequence when processing a particular element.\n",
    "\n",
    "### Intuition Behind Attention as Dot Product\n",
    "\n",
    "1. **Similarity Measurement**:  \n",
    "   - The **dot product** between two vectors (queries \\(Q\\) and keys \\(K\\)) measures their similarity or alignment.  \n",
    "   - When processing a word (or token) in a sequence, the model calculates how similar it is to other words in the same sequence. Words with higher similarity scores are deemed more relevant.\n",
    "\n",
    "2. **Context Gathering**:  \n",
    "   - Attention assigns a weight to each word (based on similarity), allowing the model to focus more on relevant words when generating the output.  \n",
    "   - For example, in the sentence \"The cat sat on the mat,\" when the model processes \"sat,\" it might pay more attention to \"cat\" because they are contextually related.\n",
    "\n",
    "3. **Scaling with Softmax**:  \n",
    "   - After computing the dot product, a softmax function normalizes the scores into probabilities. These probabilities represent how much attention should be paid to each word.\n",
    "\n",
    "4. **Dynamic Focus**:  \n",
    "   - Unlike static methods, attention dynamically computes relevance for each token in the context of the current query. This adaptability enables the model to focus on different words depending on the input.\n",
    "\n",
    "### Why Call It \"Attention\"?  \n",
    "The term \"attention\" comes from its purpose:  \n",
    "- Just like humans selectively focus on certain parts of a scene, the attention mechanism allows the model to focus on relevant parts of the input sequence while processing.\n",
    "\n",
    "### Mathematical Breakdown\n",
    "- Given \\(Q\\), \\(K\\), and \\(V\\) matrices (queries, keys, and values):  \n",
    "  $$\n",
    "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "  $$  \n",
    "- Here:\n",
    "  - \\(QK^T\\): Dot product determines how similar each query is to each key.\n",
    "  - \\(\\text{softmax}()\\): Converts these scores into a probability distribution.\n",
    "  - \\(V\\): Combines information from all values weighted by attention scores.\n",
    "\n",
    "In essence, attention via the dot product enables the model to allocate focus dynamically and contextually, forming the foundation of the Transformer architecture's power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211758d7",
   "metadata": {},
   "source": [
    "**Perplexity** is a metric used in Natural Language Processing (NLP) to evaluate how well a probabilistic model (like a language model) predicts a sequence of words. It quantifies the level of \"uncertainty\" or \"surprise\" the model has when encountering a given dataset.\n",
    "\n",
    "### Definition of Perplexity\n",
    "\n",
    "Perplexity is formally defined as the exponentiated average negative log-likelihood of a sequence. For a sequence of \\(N\\) words \\(w_1, w_2, \\ldots, w_N\\), with a language model assigning probabilities $\\(P(w_1), P(w_2 | w_1), \\ldots, P(w_N | w_1, \\ldots, w_{N-1})\\)$, the perplexity is:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 P(w_i | w_1, \\ldots, w_{i-1})}\n",
    "$$\n",
    "\n",
    "Alternatively, using natural logarithms:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = e^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i | w_1, \\ldots, w_{i-1})}\n",
    "$$\n",
    "\n",
    "### Intuition Behind Perplexity\n",
    "\n",
    "1. **Measure of Predictability**:  \n",
    "   - Lower perplexity means the model assigns higher probabilities to the actual words in the sequence, indicating better performance.\n",
    "   - Higher perplexity means the model is less confident or incorrect in its predictions.\n",
    "\n",
    "2. **Interpretation**:  \n",
    "   - Perplexity can be thought of as the \"effective size\" of the set of plausible next words according to the model.\n",
    "   - For example, if the perplexity is 10, the model effectively considers 10 equally likely words at each step.\n",
    "\n",
    "3. **Ideal Case**:  \n",
    "   - A perfect language model (that always assigns a probability of 1 to the correct word) has a perplexity of 1.\n",
    "   - A model with uniformly random predictions over \\(V\\) words in the vocabulary will have a perplexity of \\(V\\).\n",
    "\n",
    "### Practical Usage\n",
    "\n",
    "1. **Evaluation Metric**:\n",
    "   - Perplexity is commonly used to compare language models during training and testing.\n",
    "   - Lower perplexity indicates a better model.\n",
    "\n",
    "2. **Challenges**:\n",
    "   - Perplexity is sensitive to the size of the vocabulary. Models trained on larger vocabularies often have higher perplexity because probabilities are spread out over more words.\n",
    "   - It may not directly correlate with downstream task performance (e.g., translation or summarization).\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose a language model predicts the next word in a sentence, and for a sequence \\(w_1, w_2, w_3\\), it assigns probabilities:\n",
    "$$\n",
    "P(w_1) = 0.2, \\, P(w_2 | w_1) = 0.5, \\, P(w_3 | w_1, w_2) = 0.25\n",
    "$$\n",
    "Then, the perplexity for this sequence is:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = \\sqrt[3]{\\frac{1}{0.2 \\times 0.5 \\times 0.25}} = \\sqrt[3]{40} \\approx 3.42\n",
    "$$\n",
    "This means the model is, on average, as \"confused\" as if there were about 3.42 equally likely choices at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06873a31",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9493db5c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02dd81e",
   "metadata": {},
   "source": [
    "Tuning hyperparameters for transformer models can be complex, but there are some guidelines and strategies to make it more systematic and effective. Here are some tips for each parameter:\n",
    "\n",
    "### 1. **Batch Size**\n",
    "   - Larger batch sizes can speed up training, but they also r1.220673 Mequire more memory, which could be a constraint depending on your hardware (like your RTX 2060 GPU).\n",
    "   - Start with a smaller batch size (e.g., 8 or 16) and increase gradually if your GPU allows.\n",
    "   - Alternatively, gradient accumulation can mimic larger batch sizes without increasing actual memory usage.\n",
    "\n",
    "### 2. **Block Size (Sequence Length)**\n",
    "   - This controls the length of the sequence used as input. Higher values improve the model's capacity to capture long-range dependencies, but at the cost of increased memory usage.\n",
    "   - Experiment with small values (e.g., 32, 64) and increase based on your dataset's average sequence length. For very long sequences, consider breaking them up and applying techniques like masking to capture contextual information.\n",
    "\n",
    "### 3. **Learning Rate**\n",
    "   - Start with a learning rate around \\(1e-4\\) or \\(1e-3\\) and use learning rate schedulers like cosine decay or step decay.\n",
    "   - If training is unstable, reduce the learning rate, as high learning rates can cause exploding gradients in transformer models.\n",
    "   - Use smaller learning rates for fine-tuning pre-trained models.\n",
    "\n",
    "### 4. **Dropout Rate**\n",
    "   - Generally, 0.1 to 0.3 is a good range to start with. For larger datasets, lower dropout (around 0.1) may be suitable, while smaller datasets may need a higher dropout to avoid overfitting.\n",
    "\n",
    "### 5. **Embedding Size (`n_embd`)**\n",
    "   - Start with small values like 64 or 128 if your model size is restricted by hardware. However, `n_embd` should ideally be divisible by `n_head`.\n",
    "   - Increase `n_embd` if you’re observing underfitting (model is unable to learn the complexities of the dataset).\n",
    "   \n",
    "### 6. **Number of Heads (`n_head`)**\n",
    "   - Usually, `n_embd` divided by `n_head` should result in an integer for ease of implementation.\n",
    "   - For a small model, starting with 8 heads should work well. Increasing it may allow the model to capture more complex relationships but will require more memory.\n",
    "\n",
    "### 7. **Number of Layers (`n_layer`)**\n",
    "   - Start with 4–6 layers and increase as needed, depending on your hardware.\n",
    "   - More layers can improve model performance but may lead to overfitting or excessive computational load. You can also experiment with 12 layers if using techniques like regularization or dropout.\n",
    "\n",
    "### 8. **Evaluation Interval and Iterations**\n",
    "   - For `max_iters`, set it high, but use early stopping based on validation loss to prevent unnecessary computation.\n",
    "   - `eval_interval` and `eval_iters` can be adjusted to balance frequency of evaluations with performance overhead. More frequent evaluations provide more insights into learning dynamics but increase runtime.\n",
    "\n",
    "### **Practical Tips for Hyperparameter Tuning**\n",
    "- **Learning Rate Finder**: Gradually increase the learning rate and observe the loss. It can give you an idea of a good learning rate range.\n",
    "- **Grid Search or Random Search**: Start with a coarse grid or a randomized search to find promising values and then perform a finer search around these values.\n",
    "- **Optuna and Hyperopt**: Tools like Optuna can automate hyperparameter tuning by efficiently navigating the search space based on Bayesian optimization.\n",
    "\n",
    "Let me know if you’d like to dive deeper into tuning specific parameters, especially given your GPU constraints!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f177478",
   "metadata": {},
   "source": [
    "Training a transformer model, even with a smaller vocabulary size like 512, can still be slow due to the complexity of self-attention operations and the number of parameters in the model. Here are some tips to speed up training on your setup:\n",
    "\n",
    "1. **Mixed-Precision Training (FP16)**:\n",
    "   - Mixed-precision training allows computations to be done in 16-bit floating-point precision instead of 32-bit, reducing memory usage and speeding up training. NVIDIA’s **Apex library** or **PyTorch’s `torch.cuda.amp`** module can help with this. The RTX 2060 supports FP16, so this could give a performance boost.\n",
    "\n",
    "2. **Reduce Model Depth or Width**:\n",
    "   - If feasible, consider using fewer layers (reduce the depth) or fewer attention heads in the model. This will reduce the number of parameters and speed up training. For example, using a 6-layer transformer instead of a 12-layer one or reducing the number of heads from 12 to 8 could make a noticeable difference.\n",
    "\n",
    "3. **Gradient Accumulation**:\n",
    "   - If batch size is small due to memory limitations, gradient accumulation can simulate a larger batch size by accumulating gradients over multiple forward passes before updating weights. This can help with stability and possibly improve convergence speed.\n",
    "\n",
    "4. **Optimize Data Loading**:\n",
    "   - Ensure your data pipeline is efficient. Use **`DataLoader`** with `num_workers` for parallel data loading if using PyTorch, which can reduce waiting time for each batch.\n",
    "\n",
    "5. **Reduce Sequence Length (if possible)**:\n",
    "   - Training time scales with sequence length due to the self-attention mechanism. If you can shorten your input sequences without losing context, it can speed up each training iteration.\n",
    "\n",
    "6. **Increase Learning Rate or Use Warmup**:\n",
    "   - Using a slightly higher learning rate or a learning rate schedule with warmup can help the model converge faster. Be cautious, as too high of a learning rate can destabilize training.\n",
    "\n",
    "7. **Use a Smaller Batch Size with Gradient Accumulation**:\n",
    "   - If your model’s batch size is limited by memory, setting a smaller batch size and accumulating gradients over multiple steps may help.\n",
    "\n",
    "8. **Profile the Training Loop**:\n",
    "   - Tools like **PyTorch’s profiler** can help identify bottlenecks. For example, you might find that a large portion of time is spent on specific operations or data loading.\n",
    "\n",
    "Let me know if you want more details on any of these techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c016a7",
   "metadata": {},
   "source": [
    "Reducing the number of attention heads in a multihead attention mechanism doesn’t necessarily decrease the total number of model parameters because of how the parameters are allocated across different parts of the model. Here’s why this happens:\n",
    "\n",
    "1. **Fixed Projection Dimension**:\n",
    "   - In multihead attention, the total dimensionality of the attention projection is typically fixed (e.g., 512 or 768) regardless of the number of heads. For example, if the model's hidden dimension is 512, it will still remain 512 even if you reduce the number of heads. \n",
    "   - Reducing the number of heads doesn’t change the overall dimensionality of the query, key, and value matrices; instead, it just changes how the dimensionality is split across each head.\n",
    "\n",
    "2. **Parameter Distribution Across Attention Layers**:\n",
    "   - The query, key, and value matrices in the multihead attention layer are still projected to the same dimensionality, so the number of parameters in each of these matrices remains unchanged.\n",
    "   - When you reduce the number of heads, the model simply divides the same total projection size into fewer heads, making each head slightly \"wider\" but keeping the total parameter count constant.\n",
    "\n",
    "3. **Feed-Forward Layers Remain Unchanged**:\n",
    "   - In transformer layers, a significant portion of parameters is also in the feed-forward network (FFN) layer after the attention layer. This layer is independent of the number of attention heads and typically has a large number of parameters that remain the same.\n",
    "\n",
    "### How to Actually Reduce Parameters\n",
    "To reduce the number of parameters effectively, you could try the following:\n",
    "\n",
    "- **Reduce the Model’s Hidden Dimension**: Reducing the hidden dimension of the transformer (e.g., from 512 to 384) will decrease the size of both the attention and feed-forward layers, lowering parameter count overall.\n",
    "- **Reduce the Number of Layers**: Reducing the number of transformer layers (depth of the model) will decrease the number of parameters as each layer contains independent parameters.\n",
    "- **Decrease FFN Dimension**: The FFN layer often has a higher dimension than the model's hidden size (e.g., 4x the hidden dimension). Reducing this factor will also reduce the parameter count significantly.\n",
    "\n",
    "Let me know if you’d like more specifics on adjusting these aspects in your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7b0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "sys.path.append(\"/home/karapathy_trainings/\")\n",
    "# from tokenizer.minbpe.minbpe import BasicTokenizer\n",
    "import tiktoken\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "get_datetime = lambda x: f\"{x.day}_{x.month}_{x.year}_{x.hour}_{x.minute}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376a6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "126fe303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_ = BasicTokenizer()\n",
    "# tokenizer_.load(\"/home/karapathy_trainings/tokenizer/minbpe/bpe_encoding.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82d2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/hyperparam.json\", \"r\") as file:\n",
    "    hyperparams = json.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97071d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = hyperparams.get(\"batch_size\", 8) # how many independent sequences will we process in parallel?\n",
    "block_size = hyperparams.get(\"block_size\", 100) # what is the maximum context length for predictions?\n",
    "max_iters = hyperparams.get(\"max_iters\", 100)\n",
    "eval_interval = hyperparams.get(\"eval_interval\", 500)\n",
    "learning_rate = hyperparams.get(\"learning_rate\", 0.01)\n",
    "eval_iters = hyperparams.get(\"eval_iters\", 200)\n",
    "n_embd = hyperparams.get(\"n_embd\", 768) # should be a number divisible by n_head\n",
    "n_head = hyperparams.get(\"n_head\", 16) \n",
    "n_blocks = hyperparams.get(\"n_blocks\", 8)\n",
    "dropout = hyperparams.get(\"dropout\", 0.1)\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e913ff3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cuda',\n",
       " {'batch_size': 8,\n",
       "  'block_size': 32,\n",
       "  'max_iters': 3000,\n",
       "  'eval_interval': 100,\n",
       "  'learning_rate': 0.0001,\n",
       "  'eval_iters': 200,\n",
       "  'n_embd': 768,\n",
       "  'n_head': 4,\n",
       "  'n_blocks': 6,\n",
       "  'dropout': 0.3})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device, hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "359f159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae793ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = torch.randint(len(data) - block_size, (batch_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "915eed71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fdc6786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115395"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af858c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1337)\n",
    "\n",
    "# # wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# # here are all the unique characters that occur in this text\n",
    "# vocab = tokenizer_.vocab\n",
    "# vocab_size = len(vocab)\n",
    "                 \n",
    "# # Train and test splits\n",
    "# #data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# data = torch.tensor(tokenizer_.encode(text), dtype=torch.long)\n",
    "# n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "# train_data = data[:n]\n",
    "# val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bf0f069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b43c9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_details(path, hyperparams, model, loss_step_details):\n",
    "    s = get_datetime(datetime.datetime.now())\n",
    "    path_ = os.path.join(path,s)\n",
    "    if not os.path.exists(path_):\n",
    "        os.makedirs(path_)\n",
    "    root = path_\n",
    "    try:\n",
    "        with open(f\"{path_}/hyperparams.json\",\"w\") as f:\n",
    "            json.dump(hyperparams, f)\n",
    "            f.close()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        with open(f\"{path_}/loss.json\",\"w\") as f:\n",
    "            json.dump(loss_step_details, f)\n",
    "            f.close()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    torch.save(model, f\"{path_}/minigpt.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aa80be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b68e2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "#             nn.Linear(4*n_embd, 8 * n_embd),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(8*n_embd, 4 * n_embd),\n",
    "#             nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # residual connection\n",
    "        x = x + self.ffwd(self.ln2(x)) # residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b5251a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #--------------------------------------------------#\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #** Learnable Token Embedding\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd) #** Learnable Positional Encoding\n",
    "        #--------------------------------------------------#\n",
    "        \n",
    "        #-------------- Enters the transformer block ------#\n",
    "        #self.sa_head = MultiHeadAttention(n_head, n_embd//4) #Head(n_embd)\n",
    "        #self.ff = FeedFoward(n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_blocks)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        #--------------------------------------------------#\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        #x = self.sa_head(x)\n",
    "        #x = self.ff(x)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        #### No Softmax needed as cross_entropy function in pytorch internally applies\n",
    "        #### the softmax before loss calculation\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            print(\"****\",idx_cond.shape)\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            #print(idx)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1f2a9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.639425 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07382ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_curve = {}\n",
    "max_iters = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b74e2214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3607, val loss 4.3572\n",
      "step 100: train loss 2.5728, val loss 2.5842\n",
      "step 200: train loss 2.4751, val loss 2.4885\n",
      "step 300: train loss 2.3574, val loss 2.3642\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# evaluate the loss\u001b[39;00m\n\u001b[1;32m     16\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m m(xb, yb)\n",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      6\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i:i\u001b[38;5;241m+\u001b[39mblock_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[1;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([data[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39mblock_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[0;32m----> 8\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters, max_iters+1000):\n",
    "    iter_loss_temp = {}\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "\n",
    "        iter_loss_temp['train'] = float(f\"{losses['train'].item():.4f}\")\n",
    "        iter_loss_temp['val'] = float(f\"{losses['val'].item():.4f}\")\n",
    "        loss_curve[iter] = iter_loss_temp \n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "max_iters += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff732ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_curve_ = {}\n",
    "\n",
    "# for k, v in loss_curve.items():\n",
    "#     temp = {}\n",
    "#     for k1, v1 in v.items():\n",
    "#         temp[k1] = float(f\"{v1.item():.4f}\")\n",
    "#         loss_curve_[k] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae374358",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_details(\"saved_model_checkpoints\",hyperparams,m,loss_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d6d5365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loond spet in fim Vimp me-time the, teat frewn, weet sooow no login to so seseeseers'd dand sham,\n",
      "To. Mask e be we wime thalle a they woued,\n",
      "Wer go blarth and any with the le eent wuty ndow romenepals faess dious ser;\n",
      "Tre, ner on sorme in a Ist band magre,\n",
      "And com se the up to spenconce thumpe,\n",
      "Tive on sain op, congsIed cien, framese, agat, mabulave!\n",
      "Wiph, yoand faille, sabbeinl sind;\n",
      "Bept, thin reat youg fith?\n",
      "\n",
      "PUMESBES:\n",
      "Iblome ke de id asee nelce int her thowe will spes: theee swith mark swer unet the on at Wershe ,in:\n",
      "Awith they le noul meare to an cinsts deare thay?\n",
      "The sawhing thie whoy tragote stons; me,\n",
      "Theme consence on groive ist\n",
      "A stiree kne Go dest pe the passey.\n",
      "\n",
      "GLARD:\n",
      "Lin: mer him your yere beavere ansed and thise sper nearsbe neaft sperey oir the too gooow sen, en\n",
      "un weat thire Wour rou thate perty\n",
      "Re peapesee hid reade hean thatish,\n",
      "Any smaid decr thire! with thatt wowe and tham haithy.\n",
      "\n",
      "BULEY HARY EO:\n",
      "Ore, seno sirt to nieg and graves,\n",
      "Roo, note to my ly beay thom on nene\n",
      "blin boe fientire hesver, withy Ture.\n",
      "\n",
      "MPERHARK:\n",
      "Whaplol oftr hit ong thenes\n",
      "Ae dearto me int onge we nen with goone nou dio tiend shile riem cong?\n",
      "\n",
      "FROLIO:\n",
      "Prount foan iThere tlia as whe well same dres, of the ist trom.\n",
      "\n",
      "Qnty ke witonn, wall. Iy breand hind Fith.\n",
      "\n",
      "CAPTiteart, the sennd beert cane anfer buonte sies.\n",
      "\n",
      "Aeld Eakee fuock.\n",
      "\n",
      "HALUSE tCY:\n",
      "Yith crom me be.\n",
      "\n",
      "KLO:\n",
      "But Prom the yet preenmy the at spe treave. Ee gon nabely.\n",
      "\n",
      "Ding:\n",
      "Ar we by fith ste incen s made in to.\n",
      "\n",
      "SIO: fath and in un ferepen, ke nay ingedey\n",
      "Hel will thit; youbour bat. Whien?\n",
      "\n",
      "WCUSIHAgYou, OF shist hiond of ishbus toet yers,\n",
      "Creem fochseew ip cant bat leawer;\n",
      "Laty Kepustadre:\n",
      "O he aur posp, wene'll tai seare.\n",
      "\n",
      "Te,\n",
      "The lye nave you deraught poot oe be faing. Withome fatill wettruen:\n",
      "Iu dor we bete loncud right ball,\n",
      "Clone yoom prest oune, lowesvant,\n",
      "On cowk on neermese be sit not preain,\n",
      "And ise teare a mand woreroon o's.\n",
      "\n",
      "GLOd and ennProce;\n",
      "BEN\n",
      "BARK:\n",
      "Whees jande wint righode tu'try forene the nees the prees\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "torch.manual_seed(10)\n",
    "context = torch.zeros((1, 1), dtype=torch.long\n",
    "                      , device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7745e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bede99f3",
   "metadata": {},
   "source": [
    "### Inference by Loading Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96ee7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_ = torch.load(\"saved_model_checkpoints/3_11_2024_18_24/minigpt.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f3e8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"saved_model_checkpoints/3_11_2024_14_25/hyperparams.json\", \"r\") as file:\n",
    "    hyperparams = json.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "badb2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = hyperparams.get(\"batch_size\", 8) # how many independent sequences will we process in parallel?\n",
    "block_size = hyperparams.get(\"block_size\", 100) # what is the maximum context length for predictions?\n",
    "max_iters = hyperparams.get(\"max_iters\", 100)\n",
    "eval_interval = hyperparams.get(\"eval_interval\", 500)\n",
    "learning_rate = hyperparams.get(\"learning_rate\", 0.01)\n",
    "eval_iters = hyperparams.get(\"eval_iters\", 200)\n",
    "n_embd = hyperparams.get(\"n_embd\", 768) # should be a number divisible by n_head\n",
    "n_head = hyperparams.get(\"n_head\", 16) \n",
    "n_blocks = hyperparams.get(\"n_blocks\", 8)\n",
    "dropout = hyperparams.get(\"dropout\", 0.1)\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f28b808e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stry in't.\n",
      "\n",
      "JUCIO:\n",
      "Madamuch grow's tieens, a the well is with aidurest\n",
      "Shand what, much stell ger proud,\n",
      "And this, wither but I was\n",
      "That proor mann's dightio, as you ungelyout a son,\n",
      "What staaist suchorful.\n",
      "Onr Glo? A dielows thou bannot lord of an woman your wrich meliever;\n",
      "Ay, bef lie,s, merrants, but wherest of our you I but do word'd\n",
      "Date did Suntruceds. Some it, are I preace\n",
      "With coue a nature I, have king, newers on,\n",
      "Somar.' O in she that word, as may have stand if is succh.\n",
      "\n",
      "DORD Servose, Landshort of GuOUCESTER:\n",
      "Thoughost of your a mon:\n",
      "Here the wrant by everings from onithet, while\n",
      "entl my not than lad? I thangur'd I know though word fher leart's that I his garainst Tutsendy.\n",
      "\n",
      "Numpherter:\n",
      "Unge betcy\n",
      "Will, my you landed well in Here's save\n",
      "Tyser brokest.\n",
      "Come mucio, arroth thand dabe his kess word's nor tead\n",
      "So of thousandest to devinguur trestemas,\n",
      "That's by man not Morbasted?\n",
      "\n",
      "TLEORENTESS OF YORK:\n",
      "Withat, doway.\n",
      "With stard,---\n",
      "And your ventainser,\n",
      "And them'd you seture, subjes of of caer.\n",
      "And up Englite and shall himrainss;\n",
      "Who, casess as more clet with's nighinkst, fair you warger's these,\n",
      "yinke stanaces, for sleasle my\n",
      "daright froses, anot with your country\n",
      "To young fleesors boly beain, me,\n",
      "That she odne aptomple.\n",
      "\n",
      "Seet him Wearge:\n",
      "Doybelay.\n",
      "\n",
      "LAGURETER:\n",
      "Be is not?\n",
      "\n",
      "BECLIOPS:\n",
      "Ayven Vallaeul me you flaul, lord, with sood encees,\n",
      "I satke, I but chart? Buckingthal, share I'll Vent\n",
      "Omm his vonotoour, lord; Go! The eame her worne! o' crove stark's,\n",
      "Then's again wear chievent god itsedies.\n",
      "'Sest voliel.\n",
      "Farew tell, not mebid than you say voingaint\n",
      "And time anribditsly bead him long,\n",
      "The so Left, I sail thee, ha! 's swreather we\n",
      "When love such dole o' told are foi't?\n",
      "\n",
      "PEMBERLANUS:\n",
      "Pordom with high,\n",
      "Ghe oldungely'st turnticiled, the despeary\n",
      "With whencedimesomens as shephild heir\n",
      "A sseeph a? hall one maked, whan lovest of winks,\n",
      "Navele sheet of in this curtuines!\n",
      "Cal, my lord! lot\n",
      "Where the nend.\n",
      "But farmal and and word, for sir, ve thaniors,\n",
      "Tulet ovisirfess. fr\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "#torch.manual_seed(10)\n",
    "context = torch.zeros((1, 1), dtype=torch.long\n",
    "                      , device=device)\n",
    "print(decode(m_.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe006cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "696a9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"saved_model_checkpoints/3_11_2024_14_25/loss.json\", \"r\") as file:\n",
    "    loss_curve = json.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3254b17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "loss_df = pd.DataFrame(loss_curve).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a6e44be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15a2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
