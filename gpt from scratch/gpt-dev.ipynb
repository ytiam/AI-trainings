{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94ed7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial: https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319cb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82ec793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bfddc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n"
     ]
    }
   ],
   "source": [
    "print(text[:101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e313b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of unique characters in the text\n",
    "len(set(text)) #present character level vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac078b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69d27ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e41fa3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be22726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#considering simple character to ID level encoding\n",
    "stoi = {chr_:i for i, chr_ in enumerate(vocab)}\n",
    "itos = {i:chr_ for i, chr_ in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03bb2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[j] for j in s]\n",
    "decode = lambda k: ''.join([itos[m] for m in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e60098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 1, 39, 51, 1, 13, 58, 39, 52, 59]\n",
      "\n",
      "=============\n",
      "\n",
      "I am Atanu\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"I am Atanu\"))\n",
    "print(\"\\n=============\\n\")\n",
    "print(decode(encode(\"I am Atanu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f39b27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "016dd4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115395"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0425779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "732a1c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115395]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec924e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "967d1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets now split the data into train and test\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cada4306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b8ca74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b40260d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[56,  6,  0, 24, 43, 58,  1, 61],\n",
      "        [39, 47, 51,  1, 58, 46, 39, 58],\n",
      "        [52, 45,  1, 58, 53,  1, 57, 39],\n",
      "        [43, 47, 52, 45,  1, 46, 53, 50]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 6,  0, 24, 43, 58,  1, 61, 46],\n",
      "        [47, 51,  1, 58, 46, 39, 58,  1],\n",
      "        [45,  1, 58, 53,  1, 57, 39, 63],\n",
      "        [47, 52, 45,  1, 46, 53, 50, 47]])\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2bf8c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [56] the target: 6\n",
      "when input is [56, 6] the target: 0\n",
      "when input is [56, 6, 0] the target: 24\n",
      "when input is [56, 6, 0, 24] the target: 43\n",
      "when input is [56, 6, 0, 24, 43] the target: 58\n",
      "when input is [56, 6, 0, 24, 43, 58] the target: 1\n",
      "when input is [56, 6, 0, 24, 43, 58, 1] the target: 61\n",
      "when input is [56, 6, 0, 24, 43, 58, 1, 61] the target: 46\n",
      "when input is [39] the target: 47\n",
      "when input is [39, 47] the target: 51\n",
      "when input is [39, 47, 51] the target: 1\n",
      "when input is [39, 47, 51, 1] the target: 58\n",
      "when input is [39, 47, 51, 1, 58] the target: 46\n",
      "when input is [39, 47, 51, 1, 58, 46] the target: 39\n",
      "when input is [39, 47, 51, 1, 58, 46, 39] the target: 58\n",
      "when input is [39, 47, 51, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 45\n",
      "when input is [52, 45] the target: 1\n",
      "when input is [52, 45, 1] the target: 58\n",
      "when input is [52, 45, 1, 58] the target: 53\n",
      "when input is [52, 45, 1, 58, 53] the target: 1\n",
      "when input is [52, 45, 1, 58, 53, 1] the target: 57\n",
      "when input is [52, 45, 1, 58, 53, 1, 57] the target: 39\n",
      "when input is [52, 45, 1, 58, 53, 1, 57, 39] the target: 63\n",
      "when input is [43] the target: 47\n",
      "when input is [43, 47] the target: 52\n",
      "when input is [43, 47, 52] the target: 45\n",
      "when input is [43, 47, 52, 45] the target: 1\n",
      "when input is [43, 47, 52, 45, 1] the target: 46\n",
      "when input is [43, 47, 52, 45, 1, 46] the target: 53\n",
      "when input is [43, 47, 52, 45, 1, 46, 53] the target: 50\n",
      "when input is [43, 47, 52, 45, 1, 46, 53, 50] the target: 47\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d035fd67",
   "metadata": {},
   "source": [
    "The input data dimension **before embedding** for text data is not \\( (\\text{batch}, \\text{block\\_size}, \\text{vocab\\_size}) \\). That specific shape refers to a **one-hot encoded representation**, which is an intermediate representation but not the most common pre-embedding input format.\n",
    "\n",
    "Here's the clarification:\n",
    "\n",
    "### Common Input Dimension for Text Data\n",
    "- **Dimension:** \\( (B, L) \\)\n",
    "  - \\( B \\): Batch size\n",
    "  - \\( L \\): Sequence length (or block size, i.e., number of tokens in a sequence)\n",
    "- **Representation:** This format contains token IDs (integers), where each token ID corresponds to a specific entry in the vocabulary. For example:\n",
    "  ```\n",
    "  Input: [[101, 2054, 2003, 102], [101, 2129, 2024, 102]]\n",
    "  ```\n",
    "\n",
    "### One-Hot Encoding Format\n",
    "- **Dimension:** \\( (B, L, V) \\)\n",
    "  - \\( B \\): Batch size\n",
    "  - \\( L \\): Sequence length (block size)\n",
    "  - \\( V \\): Vocabulary size\n",
    "- **Representation:** Each token is represented as a one-hot vector of length \\( V \\), where only one element is 1 (the position of the token ID in the vocabulary), and the rest are 0.\n",
    "\n",
    "Example:\n",
    "If \\( V = 5 \\) and a token ID is 2, the one-hot vector is:\n",
    "```\n",
    "[0, 0, 1, 0, 0]\n",
    "```\n",
    "\n",
    "### Why One-Hot Encoding is Rarely Used Before Embedding\n",
    "1. **Memory Inefficiency:** Representing text with \\( (B, L, V) \\) requires significant memory for large vocabularies.\n",
    "2. **Embedding Layers Expect Integer IDs:** Modern transformer architectures (like BERT, GPT, etc.) expect integer token IDs \\( (B, L) \\) as input and map them to embeddings internally.\n",
    "\n",
    "### Summary\n",
    "The standard input dimension for text data **before embedding** is \\( (B, L) \\), where \\( L \\) is the sequence length. If you're referring to \\( (B, L, V) \\), that's likely a one-hot encoded intermediate representation and is not typical as the input to a transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec8ae0",
   "metadata": {},
   "source": [
    "## In the GPT2 transformer, the token embedding layer input dimension layer is 50257, but the raw input_batch dimension is [8,120]. How it is being passed at embedding layer as the dimensions are not matching?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a167047",
   "metadata": {},
   "source": [
    "The dimensions of the raw input batch and the token embedding layer in GPT-2 may seem mismatched initially, but they are entirely compatible due to how the embedding layer works. Let’s break it down:\n",
    "\n",
    "### Input Batch Dimensions: \\( [8, 120] \\)\n",
    "- **8**: Batch size — number of sequences in a batch.\n",
    "- **120**: Sequence length (tokens per sequence).\n",
    "- **Raw Input**: This is a tensor containing token IDs (integers) with values ranging from \\( 0 \\) to \\( 50256 \\) (since the vocabulary size is \\( 50257 \\)).\n",
    "\n",
    "### Token Embedding Layer Dimensions: \\( [50257, D] \\)\n",
    "- **50257**: Vocabulary size — the number of unique tokens in the tokenizer's vocabulary.\n",
    "- **\\( D \\)**: Embedding size — the dimensionality of the token embeddings (e.g., 768 or 1024 for GPT-2 small and medium).\n",
    "\n",
    "### How It Works\n",
    "The embedding layer maps each token ID in the input batch to its corresponding vector from the embedding table. Here’s the process:\n",
    "\n",
    "1. **Input Token IDs**:\n",
    "   - Each integer in the \\( [8, 120] \\) batch is treated as an index into the embedding table of size \\( [50257, D] \\).\n",
    "\n",
    "2. **Embedding Lookup**:\n",
    "   - For each token ID, the embedding layer retrieves the corresponding embedding vector from the table.\n",
    "   - The embedding lookup transforms the \\( [8, 120] \\) tensor into a new tensor of shape \\( [8, 120, D] \\), where \\( D \\) is the embedding dimension.\n",
    "\n",
    "3. **Resulting Tensor**:\n",
    "   - The resulting tensor \\( [8, 120, D] \\) is then passed to subsequent layers in the transformer.\n",
    "\n",
    "### Why Dimensions Match\n",
    "- The embedding layer expects token IDs as input, not one-hot encoded vectors. The input \\( [8, 120] \\) simply serves as indices to query the embedding table.\n",
    "- The embedding table itself is a learnable matrix of shape \\( [50257, D] \\), and each token ID directly indexes into it, producing an embedding vector of size \\( D \\).\n",
    "\n",
    "### Visualization\n",
    "1. Input batch: \n",
    "   ```\n",
    "   [[5, 17, 102, ...],  # Sequence 1\n",
    "    [23, 76, 3, ...],   # Sequence 2\n",
    "    ...\n",
    "    [12, 7, 89, ...]]   # Sequence 8\n",
    "   Shape: [8, 120]\n",
    "   ```\n",
    "\n",
    "2. Embedding table (learnable):\n",
    "   ```\n",
    "   [[e_0], [e_1], ..., [e_50256]]  # Each row is an embedding of size D\n",
    "   Shape: [50257, D]\n",
    "   ```\n",
    "\n",
    "3. Output:\n",
    "   ```\n",
    "   [[[e_5], [e_17], [e_102], ...],   # Sequence 1 embeddings\n",
    "    [[e_23], [e_76], [e_3], ...],    # Sequence 2 embeddings\n",
    "    ...\n",
    "    [[e_12], [e_7], [e_89], ...]]    # Sequence 8 embeddings\n",
    "   Shape: [8, 120, D]\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "The \"mismatch\" in dimensions is resolved because the embedding layer uses the token IDs in \\( [8, 120] \\) to index into the \\( [50257, D] \\) embedding table, producing a tensor of shape \\( [8, 120, D] \\) as output. This process is efficient and well-suited for transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5a73b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.6437, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) <- C is the embedding dimension\n",
    "        #print(logits,\"#########################\")\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "m = m.to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b796d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6800752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.687521457672119\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19cc4cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W?w3cHPyZWk,f's$a-oizCjmuX\n",
      "YoR&$FMVofXisEvB!!BA!$W:CdYlixcaeg ireeYERnkcin;lxWiHFliqmoGSKtSV&BLqWk p.SGFo.\n",
      "SGjbo!UelIlind,pea!.\n",
      "-huD3SPyckzby:CUup;MOissX3Qwty.OJlvBPUSIkyBf&patelgCIEJMk:Chll,SPlyltSPkqmoRW-wNAXQbjxCevib3s 'T:C-&dE$HZAETENehhir$Fstp-LK3:CJ-xTrg\n",
      "\n",
      "ALkOdmnunruf?qA so;;3QQkhWTE:CEt,jep$vUMUE$Ew,fMf PRD?d KISKI.JrZKINLIk!as,iyb&y&a\n",
      "SadapbWPT:VEGDxlYBTEin KNukqfa!ateyCRry ts-I&fy VE?!3Myk!qEEYFEPkURJG&y.linXy'WWhiRUFhm sEra CERWs$.-w?n;mNX&qq-w'eY.rdaJR?; s-z;K:WhsBota qHugUvxIERTI'dul\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef226e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "339315db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75eaf7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4be51766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc83ca64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2684c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "# torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03f02bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 8]), torch.Size([4, 8, 2]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.shape, xbow2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "748f0f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2534d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd0de91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f364a05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09c34557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "555f3943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "\n",
    "# Linear1(x)-> Q,  Linear2(x)-> K, Linear3(x)-> V //// For each token x\n",
    "# In some rough sense, \n",
    "# for a token--> \n",
    "#    q: what information the token looking for, \n",
    "#    k: what information it has or can contribute\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "v = value(x) # (B, T, 16)\n",
    "\n",
    "# For each token in the form of q, now checking the similarity with all other tokens in the form k\n",
    "# and it forms the weight matrix\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "# Weighted aggregation of the token and all of its previous tokens, for each token\n",
    "# this to replicate not to include later token information\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "# this to make the weight distribution positive and normal\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # \n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "# weighted aggregation\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47b6100e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06873a31",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f1240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9493db5c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02dd81e",
   "metadata": {},
   "source": [
    "Tuning hyperparameters for transformer models can be complex, but there are some guidelines and strategies to make it more systematic and effective. Here are some tips for each parameter:\n",
    "\n",
    "### 1. **Batch Size**\n",
    "   - Larger batch sizes can speed up training, but they also r1.220673 Mequire more memory, which could be a constraint depending on your hardware (like your RTX 2060 GPU).\n",
    "   - Start with a smaller batch size (e.g., 8 or 16) and increase gradually if your GPU allows.\n",
    "   - Alternatively, gradient accumulation can mimic larger batch sizes without increasing actual memory usage.\n",
    "\n",
    "### 2. **Block Size (Sequence Length)**\n",
    "   - This controls the length of the sequence used as input. Higher values improve the model's capacity to capture long-range dependencies, but at the cost of increased memory usage.\n",
    "   - Experiment with small values (e.g., 32, 64) and increase based on your dataset's average sequence length. For very long sequences, consider breaking them up and applying techniques like masking to capture contextual information.\n",
    "\n",
    "### 3. **Learning Rate**\n",
    "   - Start with a learning rate around \\(1e-4\\) or \\(1e-3\\) and use learning rate schedulers like cosine decay or step decay.\n",
    "   - If training is unstable, reduce the learning rate, as high learning rates can cause exploding gradients in transformer models.\n",
    "   - Use smaller learning rates for fine-tuning pre-trained models.\n",
    "\n",
    "### 4. **Dropout Rate**\n",
    "   - Generally, 0.1 to 0.3 is a good range to start with. For larger datasets, lower dropout (around 0.1) may be suitable, while smaller datasets may need a higher dropout to avoid overfitting.\n",
    "\n",
    "### 5. **Embedding Size (`n_embd`)**\n",
    "   - Start with small values like 64 or 128 if your model size is restricted by hardware. However, `n_embd` should ideally be divisible by `n_head`.\n",
    "   - Increase `n_embd` if you’re observing underfitting (model is unable to learn the complexities of the dataset).\n",
    "   \n",
    "### 6. **Number of Heads (`n_head`)**\n",
    "   - Usually, `n_embd` divided by `n_head` should result in an integer for ease of implementation.\n",
    "   - For a small model, starting with 8 heads should work well. Increasing it may allow the model to capture more complex relationships but will require more memory.\n",
    "\n",
    "### 7. **Number of Layers (`n_layer`)**\n",
    "   - Start with 4–6 layers and increase as needed, depending on your hardware.\n",
    "   - More layers can improve model performance but may lead to overfitting or excessive computational load. You can also experiment with 12 layers if using techniques like regularization or dropout.\n",
    "\n",
    "### 8. **Evaluation Interval and Iterations**\n",
    "   - For `max_iters`, set it high, but use early stopping based on validation loss to prevent unnecessary computation.\n",
    "   - `eval_interval` and `eval_iters` can be adjusted to balance frequency of evaluations with performance overhead. More frequent evaluations provide more insights into learning dynamics but increase runtime.\n",
    "\n",
    "### **Practical Tips for Hyperparameter Tuning**\n",
    "- **Learning Rate Finder**: Gradually increase the learning rate and observe the loss. It can give you an idea of a good learning rate range.\n",
    "- **Grid Search or Random Search**: Start with a coarse grid or a randomized search to find promising values and then perform a finer search around these values.\n",
    "- **Optuna and Hyperopt**: Tools like Optuna can automate hyperparameter tuning by efficiently navigating the search space based on Bayesian optimization.\n",
    "\n",
    "Let me know if you’d like to dive deeper into tuning specific parameters, especially given your GPU constraints!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f177478",
   "metadata": {},
   "source": [
    "Training a transformer model, even with a smaller vocabulary size like 512, can still be slow due to the complexity of self-attention operations and the number of parameters in the model. Here are some tips to speed up training on your setup:\n",
    "\n",
    "1. **Mixed-Precision Training (FP16)**:\n",
    "   - Mixed-precision training allows computations to be done in 16-bit floating-point precision instead of 32-bit, reducing memory usage and speeding up training. NVIDIA’s **Apex library** or **PyTorch’s `torch.cuda.amp`** module can help with this. The RTX 2060 supports FP16, so this could give a performance boost.\n",
    "\n",
    "2. **Reduce Model Depth or Width**:\n",
    "   - If feasible, consider using fewer layers (reduce the depth) or fewer attention heads in the model. This will reduce the number of parameters and speed up training. For example, using a 6-layer transformer instead of a 12-layer one or reducing the number of heads from 12 to 8 could make a noticeable difference.\n",
    "\n",
    "3. **Gradient Accumulation**:\n",
    "   - If batch size is small due to memory limitations, gradient accumulation can simulate a larger batch size by accumulating gradients over multiple forward passes before updating weights. This can help with stability and possibly improve convergence speed.\n",
    "\n",
    "4. **Optimize Data Loading**:\n",
    "   - Ensure your data pipeline is efficient. Use **`DataLoader`** with `num_workers` for parallel data loading if using PyTorch, which can reduce waiting time for each batch.\n",
    "\n",
    "5. **Reduce Sequence Length (if possible)**:\n",
    "   - Training time scales with sequence length due to the self-attention mechanism. If you can shorten your input sequences without losing context, it can speed up each training iteration.\n",
    "\n",
    "6. **Increase Learning Rate or Use Warmup**:\n",
    "   - Using a slightly higher learning rate or a learning rate schedule with warmup can help the model converge faster. Be cautious, as too high of a learning rate can destabilize training.\n",
    "\n",
    "7. **Use a Smaller Batch Size with Gradient Accumulation**:\n",
    "   - If your model’s batch size is limited by memory, setting a smaller batch size and accumulating gradients over multiple steps may help.\n",
    "\n",
    "8. **Profile the Training Loop**:\n",
    "   - Tools like **PyTorch’s profiler** can help identify bottlenecks. For example, you might find that a large portion of time is spent on specific operations or data loading.\n",
    "\n",
    "Let me know if you want more details on any of these techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c016a7",
   "metadata": {},
   "source": [
    "Reducing the number of attention heads in a multihead attention mechanism doesn’t necessarily decrease the total number of model parameters because of how the parameters are allocated across different parts of the model. Here’s why this happens:\n",
    "\n",
    "1. **Fixed Projection Dimension**:\n",
    "   - In multihead attention, the total dimensionality of the attention projection is typically fixed (e.g., 512 or 768) regardless of the number of heads. For example, if the model's hidden dimension is 512, it will still remain 512 even if you reduce the number of heads. \n",
    "   - Reducing the number of heads doesn’t change the overall dimensionality of the query, key, and value matrices; instead, it just changes how the dimensionality is split across each head.\n",
    "\n",
    "2. **Parameter Distribution Across Attention Layers**:\n",
    "   - The query, key, and value matrices in the multihead attention layer are still projected to the same dimensionality, so the number of parameters in each of these matrices remains unchanged.\n",
    "   - When you reduce the number of heads, the model simply divides the same total projection size into fewer heads, making each head slightly \"wider\" but keeping the total parameter count constant.\n",
    "\n",
    "3. **Feed-Forward Layers Remain Unchanged**:\n",
    "   - In transformer layers, a significant portion of parameters is also in the feed-forward network (FFN) layer after the attention layer. This layer is independent of the number of attention heads and typically has a large number of parameters that remain the same.\n",
    "\n",
    "### How to Actually Reduce Parameters\n",
    "To reduce the number of parameters effectively, you could try the following:\n",
    "\n",
    "- **Reduce the Model’s Hidden Dimension**: Reducing the hidden dimension of the transformer (e.g., from 512 to 384) will decrease the size of both the attention and feed-forward layers, lowering parameter count overall.\n",
    "- **Reduce the Number of Layers**: Reducing the number of transformer layers (depth of the model) will decrease the number of parameters as each layer contains independent parameters.\n",
    "- **Decrease FFN Dimension**: The FFN layer often has a higher dimension than the model's hidden size (e.g., 4x the hidden dimension). Reducing this factor will also reduce the parameter count significantly.\n",
    "\n",
    "Let me know if you’d like more specifics on adjusting these aspects in your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7b0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "sys.path.append(\"/home/karapathy_trainings/\")\n",
    "from tokenizer.minbpe.minbpe import BasicTokenizer\n",
    "import tiktoken\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "126fe303",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ = BasicTokenizer()\n",
    "tokenizer_.load(\"/home/karapathy_trainings/tokenizer/minbpe/bpe_encoding.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97071d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 100 # what is the maximum context length for predictions?\n",
    "max_iters = 500\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 768 # should be a number divisible by n_head\n",
    "n_head = 16 \n",
    "n_blocks = 4\n",
    "dropout = 0.2\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e913ff3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "359f159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1337)\n",
    "\n",
    "# # wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# # here are all the unique characters that occur in this text\n",
    "# chars = sorted(list(set(text)))\n",
    "# vocab_size = len(chars)\n",
    "# # create a mapping from characters to integers\n",
    "# stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "# itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "# decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "# # Train and test splits\n",
    "# data = torch.tensor(encode(text), dtype=torch.long)\n",
    "# n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "# train_data = data[:n]\n",
    "# val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af858c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "vocab = tokenizer_.vocab\n",
    "vocab_size = len(vocab)\n",
    "                 \n",
    "# Train and test splits\n",
    "#data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "data = torch.tensor(tokenizer_.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf0f069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aa80be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b68e2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(4*n_embd, 16 * n_embd),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Dropout(dropout),\n",
    "            #nn.Linear(16*n_embd, 4 * n_embd),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) # residual connection\n",
    "        x = x + self.ffwd(self.ln2(x)) # residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5251a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        #self.sa_head = MultiHeadAttention(n_head, n_embd//4) #Head(n_embd)\n",
    "        #self.ff = FeedFoward(n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_blocks)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        #x = self.sa_head(x)\n",
    "        #x = self.ff(x)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            #print(\"****\",idx_cond)\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            #print(idx)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b74e2214",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff732ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_datetime = lambda x: f\"{x.day}_{x.month}_{x.year}_{x.hour}_{x.minute}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cf8f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = get_datetime(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1788e2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = {1:{\"model_path\":s}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb61ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m,f\"model_checkpoints/minigpt_{model_directory[1]['model_path']}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ee7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_ = torch.load(\"model_checkpoints/minigpt_2_11_2024_21_13.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f28b808e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000 my ys' ell pt\n",
      "Which wardthy swouramapey ke th, sp enour was as arany of favans or leay dodr.\n",
      "\n",
      "Thenclasicor ke to youg a call selfaike.\n",
      "\n",
      "LUK:\n",
      "TI from Marre,\n",
      "KI prixck se wills and wause therf'To whim I wou ears ink my se.\n",
      "Wille our wouo's whe-thous uk'drathat a weee SAR:\n",
      "GLI HWAs and or hour somall shat raand least m frigalg'd I oftearsetts d's ofts may, wou pow the Can ble me; fuly mut ill ret ty, swarmore in the ck and sppatipyshORO:\n",
      "ABut ade isple and boall learmageld ple,\n",
      "Thaks pp the due ation. IComes of reat this indo besce with ds ely thing daidach's h and ke tos fidere of wave that mart a ck that mdower scornow 'st Fonte less.\n",
      "As fe der is'lls scan peing athougothem; Yous se thence sles ous? ce poousbe god thord\n",
      "G antegadly with ut west a idense stalame with my heee thing hearthell firue shos! Commoooingevtavise his and ter morndaking laiscracethat ghat haly\n",
      "Yethen wold mand; of prop'd as ldeaan tofice thane spare from hat stitueiparneeise ke s.\n",
      "\n",
      "Had haten hres whimy trut ce he sceight,\n",
      "And th, that sencan as that wice, to his ser.\n",
      "\n",
      "\n",
      "How on eathat f m ive ir ide in the werfent and miscan prosious me, bunewrepeaving ce\n",
      "Bul brocadot as mon den:\n",
      "Tis wavir thourusiropmust yous to bour morthematorings g whou lord,\n",
      "Lest an sediefear a but ne'd mere belf,\n",
      "Tht.\n",
      ":\n",
      "Sheathat dirie'entlight,\n",
      "FrspLity the tyourong se, stse a vereen thourine offemany so n tfes they so fecong p.\n",
      "The preeit the wereeiy ars th\n",
      "For fursiany ear; n Se as dowt obeyoul.\n",
      "Thiledeath oness\n",
      "THaprothy broan is at oldothering hedone theree.fot thert mou himeadselir devese as thou? it.\n",
      "Wme ths as hhay a me\n",
      "PEd.\n",
      "Romeset, piew as morsuch,\n",
      "Thee thon,\n",
      "Ware all f ou nese sir beast was ldo airut I d, fit\n",
      "ESThat to the recome e, swou which manclandted can cath ssing had.\n",
      "Frovell and somest ourst there, marnure d were; Iir hay h, and earselvan bobuft\n",
      "To ceies ougatton Is knothingsgrst Sirouachanes worexck ave thourumse herenne haugalsp dee nt, ghaimmiseineacks of Sominngsend\n",
      "Come pavert in and O:\n",
      "GRFilamnt,\n",
      "Whtnatey, wouyou wh, ning you, is wall. When, I r,\n",
      "Which thy ble my selxgrd pistlefes giram for entl wall d\n",
      "ROfe\n",
      "Swentrupon: mande:\n",
      "And ske ther\n",
      "Mot.\n",
      "\n",
      "Youth halite ence mpuse mest s?\n",
      "That was olforke samtsel they, wheav the speather? mooyoulornoor sey by d-umsome thoubucoman he sad inted and leassonothemanystionsels to the peir h are an with himy s,\n",
      "New and inreurr, the get? g.\n",
      "Bop,\n",
      "My thing -sling her ppace.\n",
      "\n",
      "Was them rum,\n",
      "Fe them wiufears thesearscan his forqungvis,\n",
      "Whuming shtreeavies se pon a we in mortelso's fut was ukelfight,\n",
      "Kencome d; gmay\n",
      "TI aratt. We sely and Vitvens dedid the fuit.\n",
      "O Telviful there mord:\n",
      "Maf rstill, yemversost haknothy lonso k!, and to tht seld haiders,\n",
      "The ffrested we. As ws lirem It'tore he thone agave sel mands tfous warcho.\n",
      "Set with you iolir fart is ently then tist with cors ble fewy, sle that is itherey?s.\n",
      "\n",
      "Ahord a t.\n",
      "\n",
      "The pufty looly thell ameads iou ARI will s,\n",
      "Whate\n",
      "LIAUS:\n",
      "RDok too? cLe, wheaves fut your cominsut ade n,\n",
      "Tayl, ir p h's the wougobagless\n",
      "And ENGithk op, is all sit my indelCated, th; my fre.\n",
      "\n",
      "Ep'tie thefrigent, be and bect.\n",
      "\n",
      "Eving dman it, or gere\n",
      "KES:\n",
      "The sooave whainst nracedid that I sed; not reand.\n",
      "INI OLUS:\n",
      "Do?' IFINT:\n",
      "BRI I:\n",
      "S:\n",
      "S:\n",
      "Mot cove whis \n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long\n",
    "                      , device=device)\n",
    "print(tokenizer_.decode(m_.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe006cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc53f12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a9594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254b17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6e44be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15a2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
